=== üì¶ FULL PROJECT DIRECTORY EXPORT ===
üïí Exported On: 2025-07-25 19:16:40
üìÅ Base Path: C:\Users\bindu\Desktop\Competation\google

=== üìÅ DIRECTORY STRUCTURE ===

üìÅ google/
  ‚îî‚îÄ üìÑ DEBUG_INSTRUCTIONS.md (3509 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\DEBUG_INSTRUCTIONS.md

  ‚îî‚îÄ üìÑ README.md (2732 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\README.md

  ‚îî‚îÄ üìÑ TECHNICAL_WRITEUP.md (12010 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\TECHNICAL_WRITEUP.md

  ‚îî‚îÄ üìÑ ai_response_formatter.py (9096 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_response_formatter.py

  ‚îî‚îÄ üìÑ app.log (74017 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\app.log

  ‚îî‚îÄ üìÑ app.py (9832 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\app.py

  ‚îî‚îÄ üìÑ gemma.py (11462 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\gemma.py

  ‚îî‚îÄ üìÑ index.html (23304 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\index.html

  ‚îî‚îÄ üìÑ main.py (58498 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\main.py

  ‚îî‚îÄ üìÑ ollama_config_recommendation.txt (129 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\ollama_config_recommendation.txt

  ‚îî‚îÄ üìÑ optimize_ollama.py (10258 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\optimize_ollama.py

  ‚îî‚îÄ üìÑ requirements.txt (164 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\requirements.txt

  ‚îî‚îÄ üìÑ start.bat (172 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\start.bat

  ‚îî‚îÄ üìÑ start_server.bat (187 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\start_server.bat

  ‚îî‚îÄ üìÑ user_profile_manager.py (10325 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\user_profile_manager.py

  ‚îî‚îÄ üìÑ user_profiles.json (205 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\user_profiles.json

  ‚îî‚îÄ üìÑ vedix_core.py (5979 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\vedix_core.py

  ‚îî‚îÄ üìÑ vedx_lite_prompt.txt (1532 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\vedx_lite_prompt.txt

  ‚îî‚îÄ üìÑ voice_backend.py (2527 bytes)
     üìç Location: C:\Users\bindu\Desktop\Competation\google\voice_backend.py


  üìÅ ai_modules/
    ‚îî‚îÄ üìÑ __init__.py (193 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\__init__.py

    ‚îî‚îÄ üìÑ config.py (5864 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\config.py


    üìÅ cuda_core/
      ‚îî‚îÄ üìÑ __init__.py (475 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_core\__init__.py

      ‚îî‚îÄ üìÑ cuda_manager.py (22194 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_core\cuda_manager.py


      üìÅ __pycache__/
        ‚îî‚îÄ üìÑ __init__.cpython-310.pyc (497 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_core\__pycache__\__init__.cpython-310.pyc

        ‚îî‚îÄ üìÑ cuda_manager.cpython-310.pyc (15634 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_core\__pycache__\cuda_manager.cpython-310.pyc


    üìÅ cuda_text/
      ‚îî‚îÄ üìÑ __init__.py (267 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_text\__init__.py

      ‚îî‚îÄ üìÑ cuda_text_processor.py (20143 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_text\cuda_text_processor.py


      üìÅ __pycache__/
        ‚îî‚îÄ üìÑ __init__.cpython-310.pyc (422 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_text\__pycache__\__init__.cpython-310.pyc

        ‚îî‚îÄ üìÑ cuda_text_processor.cpython-310.pyc (14703 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_text\__pycache__\cuda_text_processor.cpython-310.pyc


    üìÅ gemma_integration/
      ‚îî‚îÄ üìÑ __init__.py (364 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\__init__.py

      ‚îî‚îÄ üìÑ gemma3n_engine.py (19735 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\gemma3n_engine.py

      ‚îî‚îÄ üìÑ prompt_builder.py (8481 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\prompt_builder.py

      ‚îî‚îÄ üìÑ reasoning_layer.py (16485 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\reasoning_layer.py


      üìÅ __pycache__/
        ‚îî‚îÄ üìÑ __init__.cpython-310.pyc (529 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\__pycache__\__init__.cpython-310.pyc

        ‚îî‚îÄ üìÑ gemma3n_engine.cpython-310.pyc (13567 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\__pycache__\gemma3n_engine.cpython-310.pyc

        ‚îî‚îÄ üìÑ prompt_builder.cpython-310.pyc (6729 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\__pycache__\prompt_builder.cpython-310.pyc

        ‚îî‚îÄ üìÑ reasoning_layer.cpython-310.pyc (10706 bytes)
           üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\__pycache__\reasoning_layer.cpython-310.pyc


    üìÅ vision_cuda/
      ‚îî‚îÄ üìÑ __init__.py (269 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\vision_cuda\__init__.py

      ‚îî‚îÄ üìÑ cuda_vision_processor.py (21720 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\vision_cuda\cuda_vision_processor.py


    üìÅ __pycache__/
      ‚îî‚îÄ üìÑ __init__.cpython-310.pyc (354 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\__pycache__\__init__.cpython-310.pyc

      ‚îî‚îÄ üìÑ config.cpython-310.pyc (5010 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\__pycache__\config.cpython-310.pyc


  üìÅ logs/

  üìÅ static/

    üìÅ css/
      ‚îî‚îÄ üìÑ style.css (40249 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\css\style.css


    üìÅ js/
      ‚îî‚îÄ üìÑ advanced_memory.js (19600 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\advanced_memory.js

      ‚îî‚îÄ üìÑ enhanced_vision.js (34343 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\enhanced_vision.js

      ‚îî‚îÄ üìÑ feature_ui.js (29130 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\feature_ui.js

      ‚îî‚îÄ üìÑ feature_voice.js (11615 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\feature_voice.js

      ‚îî‚îÄ üìÑ main.js (1133 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\main.js

      ‚îî‚îÄ üìÑ request_queue.js (7415 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\request_queue.js

      ‚îî‚îÄ üìÑ script.js (0 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\script.js

      ‚îî‚îÄ üìÑ voice_settings.js (13886 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\voice_settings.js

      ‚îî‚îÄ üìÑ voice_visualizer.js (4113 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\voice_visualizer.js

      ‚îî‚îÄ üìÑ vosk-speech-recognizer.js (6302 bytes)
         üìç Location: C:\Users\bindu\Desktop\Competation\google\static\js\vosk-speech-recognizer.js


    üìÅ uploads/

  üìÅ test/
    ‚îî‚îÄ üìÑ PROJECT_OVERVIEW.md (12479 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\test\PROJECT_OVERVIEW.md

    ‚îî‚îÄ üìÑ asterisk_test.html (15073 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\test\asterisk_test.html

    ‚îî‚îÄ üìÑ chat_test.html (9319 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\test\chat_test.html

    ‚îî‚îÄ üìÑ cuda_test.py (233 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\test\cuda_test.py


  üìÅ uploads/

  üìÅ __pycache__/
    ‚îî‚îÄ üìÑ ai_response_formatter.cpython-310.pyc (6059 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\__pycache__\ai_response_formatter.cpython-310.pyc

    ‚îî‚îÄ üìÑ gemma.cpython-310.pyc (8933 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\__pycache__\gemma.cpython-310.pyc

    ‚îî‚îÄ üìÑ main.cpython-310.pyc (6667 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\__pycache__\main.cpython-310.pyc

    ‚îî‚îÄ üìÑ user_profile_manager.cpython-310.pyc (8760 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\__pycache__\user_profile_manager.cpython-310.pyc

    ‚îî‚îÄ üìÑ vedix_core.cpython-310.pyc (5194 bytes)
       üìç Location: C:\Users\bindu\Desktop\Competation\google\__pycache__\vedix_core.cpython-310.pyc


=== üêç SOURCE FILES (.py, .json) ===

üîπ 1. __init__.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\__init__.py
----- [START OF __init__.py] -----
    """
    AI Modules Package
    ==================
    This package contains various AI module integrations for the Vision Assistant.
    """
    
    __version__ = "1.0.0"
    __author__ = "Vision Assistant Team"
----- [END OF __init__.py] -----

üîπ 2. __init__.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_core\__init__.py
----- [START OF __init__.py] -----
    """
    CUDA Core Module
    ===============
    Comprehensive GPU acceleration for all AI components.
    """
    
    from .cuda_manager import (
        CudaManager, 
        get_cuda_manager, 
        cuda_available, 
        get_optimal_device, 
        clear_cuda_cache,
        CudaDeviceInfo,
        CudaMemoryInfo
    )
    
    __all__ = [
        'CudaManager',
        'get_cuda_manager', 
        'cuda_available', 
        'get_optimal_device', 
        'clear_cuda_cache',
        'CudaDeviceInfo',
        'CudaMemoryInfo'
    ]
----- [END OF __init__.py] -----

üîπ 3. __init__.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_text\__init__.py
----- [START OF __init__.py] -----
    """
    CUDA Text Processing Module
    ==========================
    GPU-accelerated text processing for enhanced performance.
    """
    
    from .cuda_text_processor import CudaTextProcessor, get_cuda_text_processor
    
    __all__ = ['CudaTextProcessor', 'get_cuda_text_processor']
----- [END OF __init__.py] -----

üîπ 4. __init__.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\__init__.py
----- [START OF __init__.py] -----
    """
    Gemma3n Integration Module
    ==========================
    Enhanced Gemma3n integration with prompt building and reasoning capabilities.
    """
    
    from .prompt_builder import GemmaPromptBuilder
    from .gemma3n_engine import Gemma3nEngine
    from .reasoning_layer import GemmaReasoningLayer
    
    __all__ = ['GemmaPromptBuilder', 'Gemma3nEngine', 'GemmaReasoningLayer']
----- [END OF __init__.py] -----

üîπ 5. __init__.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\vision_cuda\__init__.py
----- [START OF __init__.py] -----
    """
    CUDA Vision Processing Module
    ============================
    GPU-accelerated vision processing for enhanced performance.
    """
    
    from .cuda_vision_processor import CudaVisionProcessor, get_cuda_processor
    
    __all__ = ['CudaVisionProcessor', 'get_cuda_processor']
----- [END OF __init__.py] -----

üîπ 6. ai_response_formatter.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_response_formatter.py
----- [START OF ai_response_formatter.py] -----
    import re
    import logging
    from typing import Dict, Tuple, List
    
    logger = logging.getLogger(__name__)
    
    class AIResponseFormatter:
        """
        Detects and formats AI responses containing asterisk-based emphasis and actions.
        Handles cases where AI might respond with asterisks indicating emphasis, actions, or emotions.
        """
        
        def __init__(self):
            # Pattern to detect asterisk formatting (various types)
            self.asterisk_patterns = {
                'bold_italic': re.compile(r'\*\*\*(.+?)\*\*\*'),  # ***text***
                'bold': re.compile(r'\*\*(.+?)\*\*'),            # **text**
                'italic': re.compile(r'\*([^*]+?)\*'),           # *text*
                'action': re.compile(r'\*([^*]*?)\*'),           # *action* (same as italic but for actions)
                'emoji_text': re.compile(r'\*(.+?)\* (\S+)'),    # *text* emoji
            }
            
            # Common patterns that indicate AI is using asterisks for emphasis/actions
            self.ai_asterisk_indicators = [
                'Hello there! üòä',
                'I\'m here for you',
                'looking forward to',
                'glad you reached out',
                'listening ear',
                'warm presence',
                'without judgment',
                'all ears',
                'supportive and caring'
            ]
        
        def detect_asterisk_formatting(self, text: str) -> Dict[str, any]:
            """
            Detect if the text contains asterisk-based formatting typically used by AI
            
            Args:
                text (str): The AI response text to analyze
                
            Returns:
                Dict containing detection results and metadata
            """
            detection_result = {
                'has_asterisks': False,
                'asterisk_count': 0,
                'formatting_types': [],
                'detected_patterns': [],
                'is_ai_emphasis': False,
                'confidence_score': 0.0
            }
            
            # Count total asterisks
            asterisk_count = text.count('*')
            detection_result['asterisk_count'] = asterisk_count
            
            if asterisk_count == 0:
                return detection_result
            
            detection_result['has_asterisks'] = True
            
            # Check for each formatting pattern
            for pattern_name, pattern in self.asterisk_patterns.items():
                matches = pattern.findall(text)
                if matches:
                    detection_result['formatting_types'].append(pattern_name)
                    detection_result['detected_patterns'].extend(matches)
            
            # Check if this looks like AI emphasis/action formatting
            ai_indicators_found = sum(1 for indicator in self.ai_asterisk_indicators 
                                    if indicator.lower() in text.lower())
            
            # Calculate confidence score
            confidence_factors = []
            
            # Factor 1: Presence of AI-typical phrases
            if ai_indicators_found > 0:
                confidence_factors.append(0.4)
            
            # Factor 2: Multiple formatting types (suggests intentional emphasis)
            if len(detection_result['formatting_types']) > 1:
                confidence_factors.append(0.3)
            
            # Factor 3: High asterisk density
            if asterisk_count > 4:
                confidence_factors.append(0.2)
            
            # Factor 4: Emojis present (common in AI emotional responses)
            emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002700-\U000027BF\U0001F900-\U0001F9FF\U0001F018-\U0001F270]')
            if emoji_pattern.search(text):
                confidence_factors.append(0.1)
            
            detection_result['confidence_score'] = sum(confidence_factors)
            detection_result['is_ai_emphasis'] = detection_result['confidence_score'] > 0.3
            
            return detection_result
        
        def format_asterisk_response(self, text: str, output_format: str = 'html') -> str:
            """
            Convert asterisk-formatted text to proper formatting
            
            Args:
                text (str): Text with asterisk formatting
                output_format (str): 'html', 'markdown', or 'plain'
                
            Returns:
                str: Formatted text
            """
            formatted_text = text
            
            if output_format == 'html':
                # Convert in order: bold-italic first, then bold, then italic
                formatted_text = self.asterisk_patterns['bold_italic'].sub(r'<strong><em>\1</em></strong>', formatted_text)
                formatted_text = self.asterisk_patterns['bold'].sub(r'<strong>\1</strong>', formatted_text)
                formatted_text = self.asterisk_patterns['italic'].sub(r'<em>\1</em>', formatted_text)
                
            elif output_format == 'markdown':
                # Markdown already uses asterisks, so we keep them but ensure proper spacing
                formatted_text = re.sub(r'\*\*\*(.+?)\*\*\*', r'***\1***', formatted_text)
                formatted_text = re.sub(r'\*\*(.+?)\*\*', r'**\1**', formatted_text)
                formatted_text = re.sub(r'\*([^*]+?)\*', r'*\1*', formatted_text)
                
            elif output_format == 'plain':
                # Remove asterisks but preserve the emphasized text
                formatted_text = self.asterisk_patterns['bold_italic'].sub(r'\1', formatted_text)
                formatted_text = self.asterisk_patterns['bold'].sub(r'\1', formatted_text)
                formatted_text = self.asterisk_patterns['italic'].sub(r'\1', formatted_text)
                
            return formatted_text
        
        def process_ai_response(self, response_text: str, format_output: bool = True) -> Dict[str, any]:
            """
            Complete processing of AI response - detect and format asterisk usage
            
            Args:
                response_text (str): The raw AI response
                format_output (bool): Whether to format the output
                
            Returns:
                Dict containing processed response and metadata
            """
            # Detect asterisk formatting
            detection_result = self.detect_asterisk_formatting(response_text)
            
            # Format if requested and asterisks detected
            formatted_text = response_text
            if format_output and detection_result['has_asterisks']:
                formatted_text = self.format_asterisk_response(response_text, 'html')
            
            # Create processing result
            result = {
                'original_text': response_text,
                'formatted_text': formatted_text,
                'detection_result': detection_result,
                'processing_notes': []
            }
            
            # Add processing notes
            if detection_result['is_ai_emphasis']:
                result['processing_notes'].append('AI emphasis/action formatting detected')
            
            if detection_result['asterisk_count'] > 0:
                result['processing_notes'].append(f'Found {detection_result["asterisk_count"]} asterisks')
            
            if len(detection_result['formatting_types']) > 0:
                result['processing_notes'].append(f'Formatting types: {", ".join(detection_result["formatting_types"])}')
            
            return result
    
    # Example usage and testing
    def test_asterisk_detection():
        """Test the asterisk detection system with sample AI responses"""
        formatter = AIResponseFormatter()
        
        test_responses = [
            "*Hello there! üòä It's so lovely to hear from you.* ‚ú® I'm here for you, as a friend. *I'm really glad you reached out.* ü§ó",
            
            "I'm designed to be a **listening ear** and a *warm presence*. ***Is there anything on your mind you'd like to talk about?***",
            
            "Regular response without any special formatting.",
            
            "*I'll do my best* to understand and respond in a way that feels **supportive and caring**. ‚ù§Ô∏è",
            
            "***I'm really looking forward to getting to know you better!*** üíñ"
        ]
        
        print("üîç Testing AI Response Asterisk Detection System")
        print("=" * 60)
        
        for i, response in enumerate(test_responses, 1):
            print(f"\nüìù Test {i}:")
            print(f"Input: {response}")
            
            result = formatter.process_ai_response(response)
            
            print(f"‚ú® Detection Result:")
            print(f"   - Has asterisks: {result['detection_result']['has_asterisks']}")
            print(f"   - Is AI emphasis: {result['detection_result']['is_ai_emphasis']}")
            print(f"   - Confidence: {result['detection_result']['confidence_score']:.2f}")
            print(f"   - Formatting types: {result['detection_result']['formatting_types']}")
            
            if result['formatted_text'] != result['original_text']:
                print(f"üìÑ Formatted: {result['formatted_text']}")
            
            if result['processing_notes']:
                print(f"üìã Notes: {', '.join(result['processing_notes'])}")
            
            print("-" * 40)
    
    if __name__ == "__main__":
        test_asterisk_detection()
----- [END OF ai_response_formatter.py] -----

üîπ 7. app.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\app.py
----- [START OF app.py] -----
    #!/usr/bin/env python3
    """
    Simple Flask Backend for AI Assistant
    =====================================
    Handles API endpoints for chat, vision analysis, and other features.
    """
    
    from flask import Flask, request, jsonify, send_from_directory
    import os
    import base64
    import json
    from datetime import datetime
    import logging
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    app = Flask(__name__, static_folder='static', static_url_path='/static')
    
    # Simple in-memory storage (replace with database in production)
    chat_history = []
    gemma_status = {
        'available': True,  # Make available by default for demo
        'enabled': True,   # Enable by default
        'model_name': 'gemma3n:latest',
        'system_status': {'status': 'running', 'memory_usage': '2.1GB'}
    }
    
    @app.route('/')
    def index():
        """Serve the main index.html file"""
        return send_from_directory('.', 'index.html')
    
    @app.route('/<path:filename>')
    def serve_files(filename):
        """Serve static files from the root directory"""
        return send_from_directory('.', filename)
    
    @app.route('/api/chat', methods=['POST'])
    def chat():
        """Handle chat messages"""
        try:
            data = request.get_json()
            if not data:
                return jsonify({'error': 'No data provided'}), 400
            
            prompt = data.get('prompt', '')
            username = data.get('username', 'User')
            role = data.get('role', 'Friend')
            
            logger.info(f"Chat request from {username}: {prompt[:50]}...")
            
            # Store message in history
            chat_history.append({
                'timestamp': datetime.now().isoformat(),
                'username': username,
                'role': role,
                'prompt': prompt
            })
            
            # Simple AI response (replace with actual AI integration)
            if 'hello' in prompt.lower():
                response = f"Hello {username}! How can I help you today?"
            elif 'how are you' in prompt.lower():
                response = "I'm doing great, thank you for asking! How are you feeling today?"
            elif 'weather' in prompt.lower():
                response = "I don't have access to current weather data, but you can check your local weather service for accurate information."
            elif 'time' in prompt.lower():
                response = f"The current server time is {datetime.now().strftime('%H:%M:%S')}."
            else:
                response = f"I understand you said: '{prompt}'. I'm a simple demo AI assistant. In a full implementation, I would provide more helpful responses!"
            
            return jsonify({
                'success': True,
                'response': response,
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"Chat error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/analyze', methods=['POST'])
    def analyze_image():
        """Handle basic image analysis"""
        try:
            if 'image' not in request.files:
                return jsonify({'error': 'No image provided'}), 400
            
            image_file = request.files['image']
            prompt = request.form.get('prompt', 'Describe this image')
            
            logger.info(f"Image analysis request: {image_file.filename}")
            
            # In a real implementation, you would process the image here
            # For demo purposes, return a simulated response
            response = """I can see an image has been uploaded for analysis. In a full implementation, this would use:
    
    **Object Detection**: Identifying people, objects, vehicles, and other elements
    **Scene Analysis**: Understanding the environment and context  
    **Safety Assessment**: Highlighting potential hazards or obstacles
    **Spatial Relationships**: Describing where objects are positioned relative to each other
    
    For blind users, the analysis would focus on:
    - Navigation safety and obstacles
    - Important landmarks and reference points  
    - People and their activities
    - Text that might be visible
    - Overall scene context for orientation"""
            
            return jsonify({
                'success': True,
                'analysis': response,
                'response': response,
                'metadata': {
                    'processing_time': 0.5,
                    'source': 'basic_vision',
                    'template_type': 'accessibility_focused'
                }
            })
            
        except Exception as e:
            logger.error(f"Image analysis error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/enhanced-vision', methods=['POST'])
    def enhanced_vision():
        """Handle enhanced vision analysis with Gemma3n"""
        try:
            if 'image' not in request.files:
                return jsonify({'error': 'No image provided'}), 400
            
            image_file = request.files['image']
            prompt = request.form.get('prompt', 'Provide detailed visual analysis for a blind user')
            
            logger.info(f"Enhanced vision analysis: {image_file.filename}")
            
            # Simulated enhanced analysis
            response = """**Enhanced AI Vision Analysis** üß†
    
    **Scene Overview**: This appears to be an indoor/outdoor environment with multiple elements present.
    
    **Safety Analysis**: 
    - No immediate hazards detected in the foreground
    - Clear pathways visible
    - Stable ground surface
    
    **Object Detection**:
    - Various objects and structures identified
    - People may be present in the scene
    - Furniture or architectural elements visible
    
    **Navigation Guidance**:
    - Primary path appears clear ahead
    - Reference points available for orientation
    - No obstacles in immediate walking path
    
    **Additional Context**:
    This enhanced analysis would normally provide much more detailed information using advanced AI models like Gemma3n for deeper scene understanding and more accurate object recognition."""
            
            return jsonify({
                'success': True,
                'response': response,
                'enhanced_processing': True,
                'vision_description': 'Basic computer vision detected: objects, people, structures',
                'metadata': {
                    'processing_time': 1.2,
                    'source': 'enhanced_gemma3n',
                    'template_type': 'accessibility_enhanced'
                }
            })
            
        except Exception as e:
            logger.error(f"Enhanced vision error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/gemma-status', methods=['GET'])
    def get_gemma_status():
        """Get Gemma3n model status"""
        return jsonify(gemma_status)
    
    @app.route('/api/gemma-toggle', methods=['POST'])
    def toggle_gemma():
        """Toggle Gemma3n enhanced mode"""
        try:
            data = request.get_json()
            enable = data.get('enable', False)
            
            # In real implementation, this would check if Gemma3n is actually available
            gemma_status['enabled'] = enable
            gemma_status['available'] = True  # Simulate availability for demo
            
            message = f"Enhanced mode {'enabled' if enable else 'disabled'}"
            logger.info(f"Gemma toggle: {message}")
            
            return jsonify({
                'success': True,
                'enabled': enable,
                'message': message
            })
            
        except Exception as e:
            logger.error(f"Gemma toggle error: {str(e)}")
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/chat-history', methods=['GET'])
    def get_chat_history():
        """Get chat history"""
        return jsonify({
            'success': True,
            'history': chat_history[-20:]  # Return last 20 messages
        })
    
    @app.route('/api/status', methods=['GET'])
    def get_status():
        """Get server status"""
        return jsonify({
            'success': True,
            'status': 'running',
            'timestamp': datetime.now().isoformat(),
            'endpoints_available': True
        })
    
    @app.route('/api/user-fetch', methods=['GET'])
    def user_fetch():
        """Fetch user information"""
        name = request.args.get('name', 'Unknown')
        return jsonify({
            'success': True,
            'user': {
                'name': name,
                'role': 'Friend',
                'first_visit': False
            }
        })
    
    @app.route('/api/user-create', methods=['POST', 'GET'])
    def user_create():
        """Create or update user"""
        if request.method == 'POST':
            data = request.get_json() or {}
        else:
            data = {'name': request.args.get('name', 'Unknown')}
        
        name = data.get('name', 'Unknown')
        role = data.get('role', 'Friend')
        
        return jsonify({
            'success': True,
            'user': {
                'name': name,
                'role': role,
                'created': True
            }
        })
    
    @app.errorhandler(404)
    def not_found(error):
        return jsonify({'error': 'Endpoint not found'}), 404
    
    @app.errorhandler(500)
    def internal_error(error):
        return jsonify({'error': 'Internal server error'}), 500
    
    if __name__ == '__main__':
        print("üöÄ Starting AI Assistant Backend Server...")
        print("üì° Server will be available at: http://localhost:8000")
        print("üîß Available endpoints:")
        print("   - POST /api/chat - Chat with AI")
        print("   - POST /api/analyze - Basic image analysis")
        print("   - POST /api/enhanced-vision - Enhanced vision analysis")
        print("   - GET /api/gemma-status - Check Gemma3n status")
        print("   - POST /api/gemma-toggle - Toggle enhanced mode")
        print("   - GET /api/chat-history - Get chat history")
        print("üìÅ Static files served from current directory")
        print("‚ú® Ready for connections!")
        
        app.run(host='0.0.0.0', port=8000, debug=True)
----- [END OF app.py] -----

üîπ 8. config.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\config.py
----- [START OF config.py] -----
    """
    AI Modules Configuration
    ========================
    Configuration settings for AI module integrations.
    """
    
    import os
    import logging
    from typing import Dict, Any
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class AIModulesConfig:
        """Configuration manager for AI modules."""
        
        def __init__(self):
            # Gemma3n Integration Settings
            self.ENABLE_GEMMA = self._get_bool_env('ENABLE_GEMMA', True)
            self.GEMMA_MODEL_NAME = os.getenv('GEMMA_MODEL_NAME', 'gemma:2b')  # Use faster model by default
            self.GEMMA_OLLAMA_URL = os.getenv('GEMMA_OLLAMA_URL', 'http://localhost:11434')
            self.GEMMA_MAX_RETRIES = int(os.getenv('GEMMA_MAX_RETRIES', '2'))
            self.GEMMA_TIMEOUT = int(os.getenv('GEMMA_TIMEOUT', '60'))  # Increased timeout
            
            # Performance options
            self.USE_LIGHTWEIGHT_MODEL = self._get_bool_env('USE_LIGHTWEIGHT_MODEL', False)
            self.LIGHTWEIGHT_MODEL_NAME = os.getenv('LIGHTWEIGHT_MODEL_NAME', 'gemma:2b')
            
            # Logging and Debugging
            self.LOG_INTERACTIONS = self._get_bool_env('LOG_INTERACTIONS', True)
            self.DEBUG_PROMPTS = self._get_bool_env('DEBUG_PROMPTS', False)
            
            # Safety and Performance
            self.MAX_PROMPT_LENGTH = int(os.getenv('MAX_PROMPT_LENGTH', '4000'))
            self.MAX_RESPONSE_LENGTH = int(os.getenv('MAX_RESPONSE_LENGTH', '1000'))
            self.ENABLE_FALLBACK_RESPONSES = self._get_bool_env('ENABLE_FALLBACK_RESPONSES', True)
            
            # Template Settings
            self.DEFAULT_TEMPLATE_TYPE = os.getenv('DEFAULT_TEMPLATE_TYPE', 'general_assistance')
            self.VISION_TEMPLATE_TYPE = os.getenv('VISION_TEMPLATE_TYPE', 'vision_description')
            self.VOICE_TEMPLATE_TYPE = os.getenv('VOICE_TEMPLATE_TYPE', 'general_assistance')
            
            logger.info(f"AI Modules Config initialized - Gemma enabled: {self.ENABLE_GEMMA}")
        
        def _get_bool_env(self, key: str, default: bool) -> bool:
            """Get boolean environment variable."""
            value = os.getenv(key, str(default)).lower()
            return value in ('true', '1', 'yes', 'on')
        
        def toggle_gemma(self, enable: bool) -> bool:
            """Toggle Gemma3n integration on/off."""
            old_status = self.ENABLE_GEMMA
            self.ENABLE_GEMMA = enable
            logger.info(f"Gemma3n toggled from {old_status} to {enable}")
            return True
        
        def get_gemma_config(self) -> Dict[str, Any]:
            """Get Gemma3n configuration dictionary."""
            return {
                'enabled': self.ENABLE_GEMMA,
                'model_name': self.GEMMA_MODEL_NAME,
                'ollama_url': self.GEMMA_OLLAMA_URL,
                'max_retries': self.GEMMA_MAX_RETRIES,
                'timeout': self.GEMMA_TIMEOUT,
                'log_interactions': self.LOG_INTERACTIONS,
                'debug_prompts': self.DEBUG_PROMPTS,
                'max_prompt_length': self.MAX_PROMPT_LENGTH,
                'max_response_length': self.MAX_RESPONSE_LENGTH,
                'enable_fallback': self.ENABLE_FALLBACK_RESPONSES
            }
        
        def get_template_config(self) -> Dict[str, str]:
            """Get template configuration."""
            return {
                'default': self.DEFAULT_TEMPLATE_TYPE,
                'vision': self.VISION_TEMPLATE_TYPE,
                'voice': self.VOICE_TEMPLATE_TYPE
            }
        
        def validate_config(self) -> Dict[str, bool]:
            """Validate configuration settings."""
            validation = {
                'gemma_model_name_valid': bool(self.GEMMA_MODEL_NAME),
                'ollama_url_valid': self.GEMMA_OLLAMA_URL.startswith('http'),
                'retries_valid': 1 <= self.GEMMA_MAX_RETRIES <= 10,
                'timeout_valid': 5 <= self.GEMMA_TIMEOUT <= 120,
                'prompt_length_valid': 100 <= self.MAX_PROMPT_LENGTH <= 10000,
                'response_length_valid': 50 <= self.MAX_RESPONSE_LENGTH <= 5000
            }
            
            all_valid = all(validation.values())
            if not all_valid:
                logger.warning(f"Configuration validation issues found: {validation}")
            
            return validation
    
    # Global configuration instance
    _config = None
    
    def get_config() -> AIModulesConfig:
        """Get or create global configuration instance."""
        global _config
        if _config is None:
            _config = AIModulesConfig()
        return _config
    
    # Convenience functions
    def is_gemma_enabled() -> bool:
        """Check if Gemma3n is enabled."""
        return get_config().ENABLE_GEMMA
    
    def toggle_gemma_integration(enable: bool) -> bool:
        """Toggle Gemma3n integration."""
        return get_config().toggle_gemma(enable)
    
    def get_gemma_model_name() -> str:
        """Get Gemma model name."""
        return get_config().GEMMA_MODEL_NAME
    
    # Test configuration if run directly
    if __name__ == "__main__":
        config = get_config()
        print("AI Modules Configuration:")
        print("-" * 40)
        print(f"Gemma Enabled: {config.ENABLE_GEMMA}")
        print(f"Model Name: {config.GEMMA_MODEL_NAME}")
        print(f"Ollama URL: {config.GEMMA_OLLAMA_URL}")
        print(f"Max Retries: {config.GEMMA_MAX_RETRIES}")
        print(f"Timeout: {config.GEMMA_TIMEOUT}")
        print(f"Log Interactions: {config.LOG_INTERACTIONS}")
        print(f"Debug Prompts: {config.DEBUG_PROMPTS}")
        print("-" * 40)
        
        # Validate configuration
        validation = config.validate_config()
        print(f"Configuration Valid: {all(validation.values())}")
        if not all(validation.values()):
            print(f"Issues: {[k for k, v in validation.items() if not v]}")
        
        # Test toggle
        print(f"\nTesting toggle...")
        config.toggle_gemma(False)
        print(f"Gemma Enabled: {config.ENABLE_GEMMA}")
        config.toggle_gemma(True)
        print(f"Gemma Enabled: {config.ENABLE_GEMMA}")
----- [END OF config.py] -----

üîπ 9. cuda_manager.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_core\cuda_manager.py
----- [START OF cuda_manager.py] -----
    """
    CUDA Core Manager
    ================
    Comprehensive GPU acceleration for all AI components in the system.
    Manages CUDA resources, memory, and device allocation across all modules.
    """
    
    import torch
    import torch.nn.functional as F
    from torch import nn
    import logging
    import time
    import gc
    import psutil
    from typing import Dict, Any, List, Optional, Tuple, Union
    import threading
    from dataclasses import dataclass
    from contextlib import contextmanager
    import json
    
    logger = logging.getLogger(__name__)
    
    @dataclass
    class CudaDeviceInfo:
        """Information about a CUDA device."""
        index: int
        name: str
        total_memory: float  # GB
        compute_capability: Tuple[int, int]
        multiprocessor_count: int
        is_available: bool = True
    
    @dataclass
    class CudaMemoryInfo:
        """Current CUDA memory usage information."""
        allocated: float  # GB
        reserved: float   # GB
        max_allocated: float  # GB
        total: float     # GB
        free: float      # GB
        utilization: float  # Percentage
    
    class CudaManager:
        """
        Comprehensive CUDA manager for all AI operations.
        Handles device allocation, memory management, and performance optimization.
        """
        
        def __init__(self, auto_optimize: bool = True):
            """
            Initialize the CUDA manager.
            
            Args:
                auto_optimize: Whether to automatically optimize CUDA settings
            """
            self.auto_optimize = auto_optimize
            self.device_lock = threading.Lock()
            self.memory_threshold = 0.85  # Use up to 85% of GPU memory
            
            # Initialize CUDA
            self.cuda_available = torch.cuda.is_available()
            self.devices = []
            self.current_device = None
            self.default_device = None
            
            # Performance tracking
            self.stats = {
                'total_operations': 0,
                'cuda_operations': 0,
                'cpu_operations': 0,
                'memory_errors': 0,
                'device_switches': 0,
                'average_processing_time': 0.0,
                'total_memory_allocated': 0.0
            }
            
            self._initialize_cuda()
            
        def _initialize_cuda(self):
            """Initialize CUDA devices and settings."""
            if not self.cuda_available:
                logger.warning("‚ö†Ô∏è  CUDA not available - using CPU only")
                return
            
            try:
                # Get device information
                device_count = torch.cuda.device_count()
                logger.info(f"üöÄ Found {device_count} CUDA device(s)")
                
                for i in range(device_count):
                    props = torch.cuda.get_device_properties(i)
                    device_info = CudaDeviceInfo(
                        index=i,
                        name=props.name,
                        total_memory=props.total_memory / (1024**3),
                        compute_capability=(props.major, props.minor),
                        multiprocessor_count=props.multi_processor_count
                    )
                    self.devices.append(device_info)
                    
                    logger.info(f"   üì± Device {i}: {device_info.name}")
                    logger.info(f"      Memory: {device_info.total_memory:.1f} GB")
                    logger.info(f"      Compute: {device_info.compute_capability}")
                
                # Set default device (usually the first one)
                self.current_device = torch.device(f"cuda:{self.devices[0].index}")
                self.default_device = self.current_device
                torch.cuda.set_device(self.current_device)
                
                # Optimize CUDA settings
                if self.auto_optimize:
                    self._optimize_cuda_settings()
                    
                logger.info(f"‚úÖ CUDA Manager initialized on {self.current_device}")
                
            except Exception as e:
                logger.error(f"‚ùå CUDA initialization failed: {e}")
                self.cuda_available = False
        
        def _optimize_cuda_settings(self):
            """Optimize CUDA settings for best performance."""
            try:
                # Enable optimizations
                torch.backends.cudnn.enabled = True
                torch.backends.cudnn.benchmark = True  # Optimize for consistent input sizes
                torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed
                
                # Set memory management
                torch.cuda.empty_cache()
                
                # Enable mixed precision if supported
                if self._supports_mixed_precision():
                    logger.info("‚úÖ Mixed precision (FP16) support enabled")
                
                logger.info("‚ö° CUDA optimizations applied")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  CUDA optimization failed: {e}")
        
        def _supports_mixed_precision(self) -> bool:
            """Check if the device supports mixed precision (FP16)."""
            if not self.cuda_available or not self.devices:
                return False
            
            # Check compute capability (7.0+ supports Tensor Cores)
            device = self.devices[0]
            return device.compute_capability[0] >= 7
        
        @contextmanager
        def cuda_context(self, device: Optional[Union[str, torch.device]] = None):
            """
            Context manager for CUDA operations.
            
            Args:
                device: Device to use for this context
            """
            original_device = self.current_device
            
            try:
                if device is not None:
                    self.set_device(device)
                
                yield self.current_device
                
            finally:
                if device is not None and original_device != self.current_device:
                    self.set_device(original_device)
        
        def set_device(self, device: Union[str, int, torch.device]):
            """
            Set the active CUDA device.
            
            Args:
                device: Device to set as active
            """
            with self.device_lock:
                try:
                    if isinstance(device, str):
                        new_device = torch.device(device)
                    elif isinstance(device, int):
                        new_device = torch.device(f"cuda:{device}")
                    else:
                        new_device = device
                    
                    if new_device != self.current_device:
                        torch.cuda.set_device(new_device)
                        self.current_device = new_device
                        self.stats['device_switches'] += 1
                        logger.debug(f"üîÑ Switched to device: {new_device}")
                    
                except Exception as e:
                    logger.error(f"‚ùå Failed to set device {device}: {e}")
        
        def get_optimal_device(self, memory_required: float = 0.0) -> torch.device:
            """
            Get the optimal device for a task based on memory requirements.
            
            Args:
                memory_required: Memory required in GB
                
            Returns:
                Optimal device to use
            """
            if not self.cuda_available:
                return torch.device("cpu")
            
            # For now, return the default device
            # In future, could implement load balancing across multiple GPUs
            return self.default_device
        
        def get_memory_info(self, device: Optional[torch.device] = None) -> CudaMemoryInfo:
            """
            Get detailed memory information for a device.
            
            Args:
                device: Device to check (uses current device if None)
                
            Returns:
                Memory information
            """
            if not self.cuda_available:
                return CudaMemoryInfo(0, 0, 0, 0, 0, 0)
            
            device = device or self.current_device
            
            try:
                with torch.cuda.device(device):
                    allocated = torch.cuda.memory_allocated() / (1024**3)
                    reserved = torch.cuda.memory_reserved() / (1024**3)
                    max_allocated = torch.cuda.max_memory_allocated() / (1024**3)
                    
                    # Get total memory from device properties
                    device_idx = device.index if device.type == 'cuda' else 0
                    total = self.devices[device_idx].total_memory
                    free = total - allocated
                    utilization = (allocated / total) * 100 if total > 0 else 0
                    
                    return CudaMemoryInfo(
                        allocated=allocated,
                        reserved=reserved,
                        max_allocated=max_allocated,
                        total=total,
                        free=free,
                        utilization=utilization
                    )
                    
            except Exception as e:
                logger.error(f"‚ùå Failed to get memory info: {e}")
                return CudaMemoryInfo(0, 0, 0, 0, 0, 0)
        
        def check_memory_available(self, required_gb: float, device: Optional[torch.device] = None) -> bool:
            """
            Check if enough memory is available for an operation.
            
            Args:
                required_gb: Required memory in GB
                device: Device to check
                
            Returns:
                True if enough memory is available
            """
            if not self.cuda_available:
                return True  # CPU operations
            
            memory_info = self.get_memory_info(device)
            available = memory_info.free
            threshold = memory_info.total * self.memory_threshold
            
            return (memory_info.allocated + required_gb) <= threshold
        
        def clear_cache(self, device: Optional[torch.device] = None):
            """
            Clear GPU cache to free memory.
            
            Args:
                device: Device to clear cache for
            """
            if not self.cuda_available:
                return
            
            try:
                if device:
                    with torch.cuda.device(device):
                        torch.cuda.empty_cache()
                else:
                    torch.cuda.empty_cache()
                
                # Force garbage collection
                gc.collect()
                
                logger.debug("üîÑ GPU cache cleared")
                
            except Exception as e:
                logger.error(f"‚ùå Failed to clear cache: {e}")
        
        def optimize_model_for_cuda(self, model: nn.Module, use_half_precision: bool = None) -> nn.Module:
            """
            Optimize a PyTorch model for CUDA.
            
            Args:
                model: Model to optimize
                use_half_precision: Whether to use FP16 (auto-detect if None)
                
            Returns:
                Optimized model
            """
            if not self.cuda_available:
                return model
            
            try:
                # Move to GPU
                model = model.to(self.current_device)
                
                # Set to eval mode for inference
                model.eval()
                
                # Use half precision if supported and requested
                if use_half_precision is None:
                    use_half_precision = self._supports_mixed_precision()
                
                if use_half_precision:
                    model = model.half()
                    logger.debug("‚ö° Model converted to half precision (FP16)")
                
                # Compile model if available (PyTorch 2.0+)
                if hasattr(torch, 'compile'):
                    try:
                        model = torch.compile(model)
                        logger.debug("‚ö° Model compiled with torch.compile")
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è  Model compilation failed: {e}")
                
                return model
                
            except Exception as e:
                logger.error(f"‚ùå Model optimization failed: {e}")
                return model
        
        def create_tensor(self, data, dtype=None, device=None) -> torch.Tensor:
            """
            Create a tensor on the optimal device.
            
            Args:
                data: Data for tensor
                dtype: Data type
                device: Target device
                
            Returns:
                Tensor on the specified device
            """
            target_device = device or self.get_optimal_device()
            
            if isinstance(data, torch.Tensor):
                tensor = data.to(target_device)
            else:
                tensor = torch.tensor(data, device=target_device)
            
            if dtype:
                tensor = tensor.to(dtype)
            
            return tensor
        
        def profile_operation(self, operation_name: str = "operation"):
            """
            Decorator to profile CUDA operations.
            
            Args:
                operation_name: Name of the operation for logging
            """
            def decorator(func):
                def wrapper(*args, **kwargs):
                    start_time = time.time()
                    device_used = "cpu"
                    
                    try:
                        if self.cuda_available and self.current_device.type == 'cuda':
                            device_used = str(self.current_device)
                            torch.cuda.synchronize()  # Ensure GPU operations are complete
                        
                        result = func(*args, **kwargs)
                        
                        if self.cuda_available and self.current_device.type == 'cuda':
                            torch.cuda.synchronize()
                        
                        processing_time = time.time() - start_time
                        
                        # Update stats
                        self.stats['total_operations'] += 1
                        if device_used.startswith('cuda'):
                            self.stats['cuda_operations'] += 1
                        else:
                            self.stats['cpu_operations'] += 1
                        
                        # Update average processing time
                        total_ops = self.stats['total_operations']
                        current_avg = self.stats['average_processing_time']
                        self.stats['average_processing_time'] = (
                            (current_avg * (total_ops - 1) + processing_time) / total_ops
                        )
                        
                        logger.debug(f"‚ö° {operation_name}: {processing_time:.3f}s on {device_used}")
                        
                        return result
                        
                    except Exception as e:
                        logger.error(f"‚ùå {operation_name} failed: {e}")
                        raise
                        
                return wrapper
            return decorator
        
        def get_system_info(self) -> Dict[str, Any]:
            """Get comprehensive system information."""
            info = {
                'cuda_available': self.cuda_available,
                'torch_version': torch.__version__,
                'devices': [],
                'current_device': str(self.current_device) if self.current_device else None,
                'stats': self.stats.copy()
            }
            
            if self.cuda_available:
                info['cuda_version'] = torch.version.cuda
                info['cudnn_version'] = torch.backends.cudnn.version()
                info['cudnn_enabled'] = torch.backends.cudnn.enabled
                
                for device in self.devices:
                    device_info = {
                        'index': device.index,
                        'name': device.name,
                        'total_memory_gb': device.total_memory,
                        'compute_capability': device.compute_capability,
                        'multiprocessor_count': device.multiprocessor_count,
                        'memory_info': self.get_memory_info(torch.device(f"cuda:{device.index}")).__dict__
                    }
                    info['devices'].append(device_info)
            
            # Add CPU info
            info['cpu_info'] = {
                'cpu_count': psutil.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / (1024**3),
                'cpu_percent': psutil.cpu_percent()
            }
            
            return info
        
        def benchmark_device(self, device: Optional[torch.device] = None, 
                            operations: int = 1000) -> Dict[str, float]:
            """
            Benchmark a device's performance.
            
            Args:
                device: Device to benchmark
                operations: Number of operations to perform
                
            Returns:
                Benchmark results
            """
            device = device or self.current_device
            
            results = {
                'device': str(device),
                'matrix_mult_time': 0.0,
                'memory_bandwidth': 0.0,
                'tensor_ops_per_second': 0.0
            }
            
            try:
                with torch.cuda.device(device) if device.type == 'cuda' else torch.no_grad():
                    # Matrix multiplication benchmark
                    size = 1024
                    a = torch.randn(size, size, device=device)
                    b = torch.randn(size, size, device=device)
                    
                    start_time = time.time()
                    for _ in range(operations // 100):  # Fewer ops for matrix mult
                        c = torch.mm(a, b)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    results['matrix_mult_time'] = time.time() - start_time
                    
                    # Tensor operations benchmark
                    x = torch.randn(10000, device=device)
                    
                    start_time = time.time()
                    for _ in range(operations):
                        y = torch.sin(x) + torch.cos(x)
                    
                    if device.type == 'cuda':
                        torch.cuda.synchronize()
                    
                    tensor_time = time.time() - start_time
                    results['tensor_ops_per_second'] = operations / tensor_time
                    
                    logger.info(f"üìä Benchmark completed for {device}")
                    
            except Exception as e:
                logger.error(f"‚ùå Benchmark failed for {device}: {e}")
            
            return results
        
        def reset_stats(self):
            """Reset performance statistics."""
            self.stats = {
                'total_operations': 0,
                'cuda_operations': 0,
                'cpu_operations': 0,
                'memory_errors': 0,
                'device_switches': 0,
                'average_processing_time': 0.0,
                'total_memory_allocated': 0.0
            }
            
            if self.cuda_available:
                torch.cuda.reset_peak_memory_stats()
        
        def health_check(self) -> Dict[str, Any]:
            """Perform a comprehensive health check."""
            health = {
                'overall_status': 'healthy',
                'cuda_status': 'ok' if self.cuda_available else 'unavailable',
                'memory_status': 'ok',
                'performance_status': 'ok',
                'issues': []
            }
            
            if self.cuda_available:
                # Check memory usage
                memory_info = self.get_memory_info()
                if memory_info.utilization > 90:
                    health['memory_status'] = 'warning'
                    health['issues'].append(f"High GPU memory usage: {memory_info.utilization:.1f}%")
                
                # Check for memory errors
                if self.stats['memory_errors'] > 0:
                    health['memory_status'] = 'error'
                    health['issues'].append(f"Memory errors detected: {self.stats['memory_errors']}")
                
                # Check performance
                if self.stats['total_operations'] > 0:
                    cuda_ratio = self.stats['cuda_operations'] / self.stats['total_operations']
                    if cuda_ratio < 0.5:
                        health['performance_status'] = 'warning'
                        health['issues'].append(f"Low GPU utilization: {cuda_ratio:.2%}")
            
            if health['issues']:
                health['overall_status'] = 'warning' if health['memory_status'] != 'error' else 'error'
            
            return health
    
    # Global CUDA manager instance
    _cuda_manager = None
    
    def get_cuda_manager(auto_optimize: bool = True) -> CudaManager:
        """Get or create the global CUDA manager."""
        global _cuda_manager
        if _cuda_manager is None:
            _cuda_manager = CudaManager(auto_optimize=auto_optimize)
        return _cuda_manager
    
    def cuda_available() -> bool:
        """Check if CUDA is available."""
        return get_cuda_manager().cuda_available
    
    def get_optimal_device() -> torch.device:
        """Get the optimal device for operations."""
        return get_cuda_manager().get_optimal_device()
    
    def clear_cuda_cache():
        """Clear CUDA cache."""
        get_cuda_manager().clear_cache()
    
    # Test the CUDA manager if run directly
    if __name__ == "__main__":
        print("üß™ Testing CUDA Manager...")
        
        manager = CudaManager()
        system_info = manager.get_system_info()
        
        print(f"üìä System Info:")
        print(f"   CUDA Available: {system_info['cuda_available']}")
        print(f"   PyTorch: {system_info['torch_version']}")
        
        if system_info['cuda_available']:
            print(f"   CUDA: {system_info['cuda_version']}")
            print(f"   Devices: {len(system_info['devices'])}")
            for device in system_info['devices']:
                print(f"      {device['name']}: {device['total_memory_gb']:.1f} GB")
        
        # Run benchmark
        if manager.cuda_available:
            print("\nüèÉ Running benchmark...")
            benchmark = manager.benchmark_device()
            print(f"   Matrix multiplication: {benchmark['matrix_mult_time']:.3f}s")
            print(f"   Tensor ops/sec: {benchmark['tensor_ops_per_second']:.0f}")
        
        # Health check
        health = manager.health_check()
        print(f"\nüè• Health: {health['overall_status']}")
        if health['issues']:
            for issue in health['issues']:
                print(f"   ‚ö†Ô∏è  {issue}")
        
        print("‚úÖ CUDA Manager test complete!")
----- [END OF cuda_manager.py] -----

üîπ 10. cuda_test.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\test\cuda_test.py
----- [START OF cuda_test.py] -----
    import torch
    
    if torch.cuda.is_available():
        print("CUDA is available")
        print("Device Name:", torch.cuda.get_device_name(0))
        print("Total GPUs:", torch.cuda.device_count())
    else:
        print("CUDA is NOT available")
----- [END OF cuda_test.py] -----

üîπ 11. cuda_text_processor.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\cuda_text\cuda_text_processor.py
----- [START OF cuda_text_processor.py] -----
    """
    CUDA-Accelerated Text Processing
    ===============================
    GPU-accelerated text generation, embeddings, and language processing.
    """
    
    import torch
    import torch.nn.functional as F
    from torch import nn
    import numpy as np
    import logging
    import time
    from typing import Dict, Any, List, Optional, Tuple, Union
    import requests
    import json
    from transformers import AutoTokenizer, AutoModel, pipeline
    from sentence_transformers import SentenceTransformer
    
    from ..cuda_core import get_cuda_manager
    
    logger = logging.getLogger(__name__)
    
    class CudaTextProcessor:
        """
        CUDA-accelerated text processing for all language tasks.
        Includes embeddings, similarity, sentiment analysis, and more.
        """
        
        def __init__(self, device: str = "auto"):
            """
            Initialize the CUDA text processor.
            
            Args:
                device: Device to use ('auto', 'cuda', 'cpu')
            """
            self.cuda_manager = get_cuda_manager()
            self.device = self.cuda_manager.get_optimal_device()
            
            # Models
            self.models = {}
            self.tokenizers = {}
            self.pipelines = {}
            
            # Performance tracking
            self.stats = {
                'embeddings_generated': 0,
                'texts_processed': 0,
                'average_processing_time': 0,
                'total_tokens_processed': 0
            }
            
            logger.info(f"CudaTextProcessor initialized on {self.device}")
            self._initialize_models()
        
        def _initialize_models(self):
            """Initialize text processing models on GPU."""
            logger.info("üîÑ Loading text processing models...")
            
            try:
                # 1. Sentence embeddings model
                self.models['embeddings'] = SentenceTransformer('all-MiniLM-L6-v2')
                if self.cuda_manager.cuda_available:
                    self.models['embeddings'] = self.models['embeddings'].to(self.device)
                logger.info("‚úÖ Sentence embeddings model loaded")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to load embeddings model: {e}")
            
            try:
                # 2. Sentiment analysis pipeline
                self.pipelines['sentiment'] = pipeline(
                    "sentiment-analysis",
                    model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                    device=0 if self.cuda_manager.cuda_available else -1
                )
                logger.info("‚úÖ Sentiment analysis pipeline loaded")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to load sentiment model: {e}")
            
            try:
                # 3. Text summarization pipeline
                self.pipelines['summarization'] = pipeline(
                    "summarization",
                    model="facebook/bart-large-cnn",
                    device=0 if self.cuda_manager.cuda_available else -1
                )
                logger.info("‚úÖ Text summarization pipeline loaded")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to load summarization model: {e}")
            
            try:
                # 4. Question answering pipeline
                self.pipelines['qa'] = pipeline(
                    "question-answering",
                    model="distilbert-base-cased-distilled-squad",
                    device=0 if self.cuda_manager.cuda_available else -1
                )
                logger.info("‚úÖ Question answering pipeline loaded")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è  Failed to load QA model: {e}")
            
            # Clear cache after loading
            if self.cuda_manager.cuda_available:
                self.cuda_manager.clear_cache()
        
        @get_cuda_manager().profile_operation("text_embedding")
        def generate_embeddings(self, texts: Union[str, List[str]], 
                              normalize: bool = True) -> torch.Tensor:
            """
            Generate embeddings for text(s) using GPU acceleration.
            
            Args:
                texts: Text or list of texts to embed
                normalize: Whether to normalize embeddings
                
            Returns:
                Tensor of embeddings
            """
            if 'embeddings' not in self.models:
                raise ValueError("Embeddings model not available")
            
            start_time = time.time()
            
            try:
                with self.cuda_manager.cuda_context():
                    # Convert single text to list
                    if isinstance(texts, str):
                        texts = [texts]
                    
                    # Generate embeddings
                    embeddings = self.models['embeddings'].encode(
                        texts,
                        convert_to_tensor=True,
                        device=self.device
                    )
                    
                    # Normalize if requested
                    if normalize:
                        embeddings = F.normalize(embeddings, p=2, dim=1)
                    
                    # Update stats
                    self.stats['embeddings_generated'] += len(texts)
                    self.stats['texts_processed'] += len(texts)
                    
                    processing_time = time.time() - start_time
                    self._update_processing_time(processing_time)
                    
                    return embeddings
                    
            except Exception as e:
                logger.error(f"‚ùå Embedding generation failed: {e}")
                raise
        
        @get_cuda_manager().profile_operation("text_similarity")
        def compute_similarity(self, text1: Union[str, torch.Tensor], 
                              text2: Union[str, torch.Tensor],
                              metric: str = "cosine") -> float:
            """
            Compute similarity between two texts or embeddings.
            
            Args:
                text1: First text or embedding
                text2: Second text or embedding
                metric: Similarity metric ('cosine', 'euclidean', 'dot')
                
            Returns:
                Similarity score
            """
            try:
                # Generate embeddings if needed
                if isinstance(text1, str):
                    emb1 = self.generate_embeddings(text1)
                else:
                    emb1 = text1
                
                if isinstance(text2, str):
                    emb2 = self.generate_embeddings(text2)
                else:
                    emb2 = text2
                
                # Ensure tensors are on the same device
                emb1 = emb1.to(self.device)
                emb2 = emb2.to(self.device)
                
                # Compute similarity
                if metric == "cosine":
                    similarity = F.cosine_similarity(emb1, emb2, dim=-1)
                elif metric == "euclidean":
                    similarity = -torch.norm(emb1 - emb2, dim=-1)  # Negative for similarity
                elif metric == "dot":
                    similarity = torch.sum(emb1 * emb2, dim=-1)
                else:
                    raise ValueError(f"Unknown metric: {metric}")
                
                return float(similarity.cpu().item())
                
            except Exception as e:
                logger.error(f"‚ùå Similarity computation failed: {e}")
                raise
        
        @get_cuda_manager().profile_operation("semantic_search")
        def semantic_search(self, query: str, documents: List[str], 
                           top_k: int = 5) -> List[Dict[str, Any]]:
            """
            Perform semantic search using GPU-accelerated embeddings.
            
            Args:
                query: Search query
                documents: List of documents to search
                top_k: Number of top results to return
                
            Returns:
                List of search results with scores
            """
            try:
                # Generate embeddings
                query_emb = self.generate_embeddings(query)
                doc_embeddings = self.generate_embeddings(documents)
                
                # Compute similarities
                similarities = F.cosine_similarity(
                    query_emb.unsqueeze(0), 
                    doc_embeddings, 
                    dim=-1
                )
                
                # Get top-k results
                top_scores, top_indices = torch.topk(similarities, min(top_k, len(documents)))
                
                results = []
                for score, idx in zip(top_scores.cpu().numpy(), top_indices.cpu().numpy()):
                    results.append({
                        'document': documents[idx],
                        'score': float(score),
                        'index': int(idx)
                    })
                
                return results
                
            except Exception as e:
                logger.error(f"‚ùå Semantic search failed: {e}")
                raise
        
        @get_cuda_manager().profile_operation("sentiment_analysis")
        def analyze_sentiment(self, text: str) -> Dict[str, Any]:
            """
            Analyze sentiment of text using GPU acceleration.
            
            Args:
                text: Text to analyze
                
            Returns:
                Sentiment analysis results
            """
            if 'sentiment' not in self.pipelines:
                # Fallback to simple rule-based sentiment
                return self._simple_sentiment(text)
            
            try:
                result = self.pipelines['sentiment'](text)
                
                return {
                    'label': result[0]['label'],
                    'score': result[0]['score'],
                    'confidence': result[0]['score'],
                    'processing_device': str(self.device)
                }
                
            except Exception as e:
                logger.error(f"‚ùå Sentiment analysis failed: {e}")
                return self._simple_sentiment(text)
        
        def _simple_sentiment(self, text: str) -> Dict[str, Any]:
            """Simple rule-based sentiment analysis fallback."""
            positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic']
            negative_words = ['bad', 'terrible', 'awful', 'horrible', 'disappointing', 'sad']
            
            text_lower = text.lower()
            pos_count = sum(1 for word in positive_words if word in text_lower)
            neg_count = sum(1 for word in negative_words if word in text_lower)
            
            if pos_count > neg_count:
                return {'label': 'POSITIVE', 'score': 0.7, 'confidence': 0.7}
            elif neg_count > pos_count:
                return {'label': 'NEGATIVE', 'score': 0.7, 'confidence': 0.7}
            else:
                return {'label': 'NEUTRAL', 'score': 0.5, 'confidence': 0.5}
        
        @get_cuda_manager().profile_operation("text_summarization")
        def summarize_text(self, text: str, max_length: int = 150, 
                          min_length: int = 30) -> str:
            """
            Summarize text using GPU acceleration.
            
            Args:
                text: Text to summarize
                max_length: Maximum summary length
                min_length: Minimum summary length
                
            Returns:
                Summarized text
            """
            if 'summarization' not in self.pipelines:
                # Fallback to simple truncation
                sentences = text.split('. ')
                return '. '.join(sentences[:3]) + '...' if len(sentences) > 3 else text
            
            try:
                # Truncate if text is too long
                if len(text) > 1000:
                    text = text[:1000] + "..."
                
                result = self.pipelines['summarization'](
                    text,
                    max_length=max_length,
                    min_length=min_length,
                    do_sample=False
                )
                
                return result[0]['summary_text']
                
            except Exception as e:
                logger.error(f"‚ùå Text summarization failed: {e}")
                return text[:200] + "..." if len(text) > 200 else text
        
        @get_cuda_manager().profile_operation("question_answering")
        def answer_question(self, question: str, context: str) -> Dict[str, Any]:
            """
            Answer a question based on context using GPU acceleration.
            
            Args:
                question: Question to answer
                context: Context containing the answer
                
            Returns:
                Answer with confidence score
            """
            if 'qa' not in self.pipelines:
                return {
                    'answer': "Question answering model not available",
                    'score': 0.0,
                    'start': 0,
                    'end': 0
                }
            
            try:
                result = self.pipelines['qa'](question=question, context=context)
                
                return {
                    'answer': result['answer'],
                    'score': result['score'],
                    'start': result['start'],
                    'end': result['end'],
                    'processing_device': str(self.device)
                }
                
            except Exception as e:
                logger.error(f"‚ùå Question answering failed: {e}")
                return {
                    'answer': "Unable to process question",
                    'score': 0.0,
                    'start': 0,
                    'end': 0
                }
        
        def extract_keywords(self, text: str, top_k: int = 10) -> List[Dict[str, Any]]:
            """
            Extract keywords from text using simple frequency analysis.
            
            Args:
                text: Text to extract keywords from
                top_k: Number of top keywords to return
                
            Returns:
                List of keywords with scores
            """
            import re
            from collections import Counter
            
            # Simple keyword extraction
            words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())
            
            # Remove common stop words
            stop_words = {'the', 'and', 'but', 'for', 'are', 'with', 'this', 'that', 'have', 'will'}
            words = [word for word in words if word not in stop_words]
            
            # Count frequencies
            word_counts = Counter(words)
            total_words = len(words)
            
            keywords = []
            for word, count in word_counts.most_common(top_k):
                keywords.append({
                    'keyword': word,
                    'frequency': count,
                    'score': count / total_words
                })
            
            return keywords
        
        def batch_process_texts(self, texts: List[str], 
                              operations: List[str] = ['embeddings', 'sentiment'],
                              batch_size: int = 32) -> List[Dict[str, Any]]:
            """
            Process multiple texts in batches for efficiency.
            
            Args:
                texts: List of texts to process
                operations: Operations to perform on each text
                batch_size: Size of processing batches
                
            Returns:
                List of processing results
            """
            results = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                batch_results = []
                
                for text in batch:
                    text_result = {'text': text}
                    
                    try:
                        if 'embeddings' in operations:
                            text_result['embedding'] = self.generate_embeddings(text)
                        
                        if 'sentiment' in operations:
                            text_result['sentiment'] = self.analyze_sentiment(text)
                        
                        if 'keywords' in operations:
                            text_result['keywords'] = self.extract_keywords(text)
                        
                        if 'summary' in operations and len(text) > 100:
                            text_result['summary'] = self.summarize_text(text)
                        
                    except Exception as e:
                        logger.error(f"‚ùå Failed to process text: {e}")
                        text_result['error'] = str(e)
                    
                    batch_results.append(text_result)
                
                results.extend(batch_results)
                
                # Clear cache between batches
                if self.cuda_manager.cuda_available:
                    self.cuda_manager.clear_cache()
            
            return results
        
        def _update_processing_time(self, processing_time: float):
            """Update average processing time statistics."""
            total_ops = self.stats['texts_processed']
            current_avg = self.stats['average_processing_time']
            
            if total_ops > 0:
                self.stats['average_processing_time'] = (
                    (current_avg * (total_ops - 1) + processing_time) / total_ops
                )
            else:
                self.stats['average_processing_time'] = processing_time
        
        def get_stats(self) -> Dict[str, Any]:
            """Get processing statistics."""
            stats = self.stats.copy()
            stats['device'] = str(self.device)
            stats['models_loaded'] = list(self.models.keys()) + list(self.pipelines.keys())
            stats['cuda_available'] = self.cuda_manager.cuda_available
            
            if self.cuda_manager.cuda_available:
                memory_info = self.cuda_manager.get_memory_info()
                stats['memory_usage'] = memory_info.__dict__
            
            return stats
        
        def clear_cache(self):
            """Clear GPU cache and reset memory."""
            self.cuda_manager.clear_cache()
        
        def benchmark(self, sample_texts: List[str] = None) -> Dict[str, float]:
            """Benchmark text processing performance."""
            if not sample_texts:
                sample_texts = [
                    "This is a sample text for benchmarking.",
                    "GPU acceleration makes text processing much faster.",
                    "CUDA enables parallel processing of multiple texts simultaneously."
                ]
            
            results = {}
            
            # Benchmark embeddings
            start_time = time.time()
            for text in sample_texts:
                self.generate_embeddings(text)
            results['embeddings_per_second'] = len(sample_texts) / (time.time() - start_time)
            
            # Benchmark sentiment analysis
            start_time = time.time()
            for text in sample_texts:
                self.analyze_sentiment(text)
            results['sentiment_per_second'] = len(sample_texts) / (time.time() - start_time)
            
            # Benchmark similarity
            start_time = time.time()
            for i in range(len(sample_texts) - 1):
                self.compute_similarity(sample_texts[i], sample_texts[i + 1])
            results['similarity_per_second'] = (len(sample_texts) - 1) / (time.time() - start_time)
            
            return results
    
    # Global instance for easy access
    _cuda_text_processor = None
    
    def get_cuda_text_processor(device: str = "auto") -> CudaTextProcessor:
        """Get or create global CUDA text processor instance."""
        global _cuda_text_processor
        if _cuda_text_processor is None:
            _cuda_text_processor = CudaTextProcessor(device=device)
        return _cuda_text_processor
    
    # Test the processor if run directly
    if __name__ == "__main__":
        print("üß™ Testing CUDA Text Processor...")
        
        processor = CudaTextProcessor()
        
        # Test embeddings
        text = "Hello, this is a test for GPU-accelerated text processing!"
        embedding = processor.generate_embeddings(text)
        print(f"üìä Generated embedding shape: {embedding.shape}")
        
        # Test sentiment
        sentiment = processor.analyze_sentiment(text)
        print(f"üòä Sentiment: {sentiment}")
        
        # Test similarity
        text2 = "Hi, this is another test for GPU text processing!"
        similarity = processor.compute_similarity(text, text2)
        print(f"üîó Similarity: {similarity:.3f}")
        
        # Get stats
        stats = processor.get_stats()
        print(f"üìà Stats: {stats}")
        
        print("‚úÖ CUDA Text Processor test complete!")
----- [END OF cuda_text_processor.py] -----

üîπ 12. cuda_vision_processor.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\vision_cuda\cuda_vision_processor.py
----- [START OF cuda_vision_processor.py] -----
    """
    CUDA-Accelerated Vision Processor
    =================================
    GPU-accelerated vision processing using PyTorch and transformers.
    Provides fast image analysis for blind users with safety-focused descriptions.
    """
    
    import torch
    import torch.nn.functional as F
    from torch import nn
    import torchvision.transforms as transforms
    from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights
    from PIL import Image
    import numpy as np
    import logging
    import time
    from typing import Dict, Any, List, Optional, Tuple
    import json
    import cv2
    
    # Try to import additional vision models
    try:
        from transformers import BlipProcessor, BlipForConditionalGeneration
        from transformers import CLIPProcessor, CLIPModel
        TRANSFORMERS_AVAILABLE = True
    except ImportError:
        print("‚ö†Ô∏è  transformers not available - install with: pip install transformers")
        TRANSFORMERS_AVAILABLE = False
    
    try:
        import ultralytics
        from ultralytics import YOLO
        YOLO_AVAILABLE = True
    except ImportError:
        print("‚ö†Ô∏è  ultralytics not available - install with: pip install ultralytics")
        YOLO_AVAILABLE = False
    
    logger = logging.getLogger(__name__)
    
    class CudaVisionProcessor:
        """
        CUDA-accelerated vision processor for blind assistance.
        Combines multiple vision models for comprehensive scene analysis.
        """
        
        def __init__(self, device: str = "auto", model_cache_dir: str = "models/"):
            """
            Initialize the CUDA vision processor.
            
            Args:
                device: Device to use ('auto', 'cuda', 'cpu')
                model_cache_dir: Directory to cache downloaded models
            """
            self.device = self._setup_device(device)
            self.model_cache_dir = model_cache_dir
            
            # Performance tracking
            self.stats = {
                'total_processed': 0,
                'average_processing_time': 0,
                'cuda_memory_used': 0,
                'last_processing_time': 0
            }
            
            # Initialize models
            self.models = {}
            self.processors = {}
            self.transforms = self._setup_transforms()
            
            logger.info(f"CudaVisionProcessor initialized on {self.device}")
            self._initialize_models()
        
        def _setup_device(self, device: str) -> torch.device:
            """Setup the best available device."""
            if device == "auto":
                if torch.cuda.is_available():
                    device = "cuda"
                    logger.info(f"üöÄ CUDA detected: {torch.cuda.get_device_name(0)}")
                    logger.info(f"üîã CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
                else:
                    device = "cpu"
                    logger.warning("‚ö†Ô∏è  CUDA not available, using CPU")
            
            device = torch.device(device)
            
            # Set optimal settings for CUDA
            if device.type == "cuda":
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            return device
        
        def _setup_transforms(self) -> Dict[str, transforms.Compose]:
            """Setup image transformations for different models."""
            return {
                'efficient_net': transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                       std=[0.229, 0.224, 0.225])
                ]),
                'clip': transforms.Compose([
                    transforms.Resize((224, 224)),
                    transforms.ToTensor(),
                    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],
                                       std=[0.26862954, 0.26130258, 0.27577711])
                ])
            }
        
        def _initialize_models(self):
            """Initialize all vision models on GPU."""
            logger.info("üîÑ Loading vision models...")
            
            # 1. EfficientNet for general object detection
            try:
                self.models['efficientnet'] = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)
                self.models['efficientnet'].eval()
                self.models['efficientnet'] = self.models['efficientnet'].to(self.device)
                logger.info("‚úÖ EfficientNet loaded")
            except Exception as e:
                logger.error(f"‚ùå Failed to load EfficientNet: {e}")
            
            # 2. BLIP for image captioning (if available)
            if TRANSFORMERS_AVAILABLE:
                try:
                    self.processors['blip'] = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
                    self.models['blip'] = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
                    self.models['blip'] = self.models['blip'].to(self.device)
                    logger.info("‚úÖ BLIP image captioning loaded")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Failed to load BLIP: {e}")
            
            # 3. CLIP for scene understanding (if available)
            if TRANSFORMERS_AVAILABLE:
                try:
                    self.processors['clip'] = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
                    self.models['clip'] = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
                    self.models['clip'] = self.models['clip'].to(self.device)
                    logger.info("‚úÖ CLIP model loaded")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Failed to load CLIP: {e}")
            
            # 4. YOLO for object detection (if available)
            if YOLO_AVAILABLE:
                try:
                    self.models['yolo'] = YOLO('yolov8n.pt')  # Nano version for speed
                    if self.device.type == 'cuda':
                        self.models['yolo'].to(self.device)
                    logger.info("‚úÖ YOLOv8 object detection loaded")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  Failed to load YOLO: {e}")
            
            # Clear GPU cache after loading
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
        
        def process_image(self, image_input, analysis_type: str = "comprehensive") -> Dict[str, Any]:
            """
            Process an image with GPU acceleration.
            
            Args:
                image_input: PIL Image, numpy array, or file path
                analysis_type: Type of analysis ('comprehensive', 'safety', 'navigation', 'objects')
            
            Returns:
                Dictionary with analysis results
            """
            start_time = time.time()
            
            try:
                # Convert input to PIL Image
                if isinstance(image_input, str):
                    image = Image.open(image_input).convert('RGB')
                elif isinstance(image_input, np.ndarray):
                    image = Image.fromarray(image_input).convert('RGB')
                elif hasattr(image_input, 'read'):  # File-like object
                    image = Image.open(image_input).convert('RGB')
                else:
                    image = image_input.convert('RGB')
                
                # Run analysis based on type
                results = {}
                
                if analysis_type in ["comprehensive", "all"]:
                    results.update(self._comprehensive_analysis(image))
                elif analysis_type == "safety":
                    results.update(self._safety_analysis(image))
                elif analysis_type == "navigation":
                    results.update(self._navigation_analysis(image))
                elif analysis_type == "objects":
                    results.update(self._object_detection(image))
                else:
                    results.update(self._basic_analysis(image))
                
                # Update stats
                processing_time = time.time() - start_time
                self._update_stats(processing_time)
                
                results['processing_info'] = {
                    'processing_time': round(processing_time, 3),
                    'device_used': str(self.device),
                    'cuda_available': torch.cuda.is_available(),
                    'memory_used': self._get_memory_usage()
                }
                
                return results
                
            except Exception as e:
                logger.error(f"Error processing image: {e}")
                return {
                    'error': str(e),
                    'fallback_description': "Unable to process image with GPU acceleration. Please try again."
                }
        
        def _comprehensive_analysis(self, image: Image.Image) -> Dict[str, Any]:
            """Run comprehensive analysis using all available models."""
            results = {
                'description': "",
                'objects': [],
                'safety_assessment': "",
                'navigation_guidance': "",
                'scene_context': ""
            }
            
            # 1. Image captioning with BLIP
            if 'blip' in self.models:
                caption = self._generate_caption(image)
                results['description'] = caption
            
            # 2. Object detection with YOLO
            if 'yolo' in self.models:
                objects = self._detect_objects_yolo(image)
                results['objects'] = objects
            
            # 3. Scene classification with EfficientNet
            if 'efficientnet' in self.models:
                scene_info = self._classify_scene(image)
                results['scene_context'] = scene_info
            
            # 4. Safety and navigation analysis
            results['safety_assessment'] = self._analyze_safety(results['objects'])
            results['navigation_guidance'] = self._generate_navigation_guidance(results['objects'])
            
            # 5. Generate comprehensive description
            results['comprehensive_description'] = self._generate_comprehensive_description(results)
            
            return results
        
        def _generate_caption(self, image: Image.Image) -> str:
            """Generate image caption using BLIP."""
            try:
                if 'blip' not in self.models:
                    return "Image captioning not available"
                
                with torch.no_grad():
                    inputs = self.processors['blip'](image, return_tensors="pt").to(self.device)
                    out = self.models['blip'].generate(**inputs, max_new_tokens=100, num_beams=5)
                    caption = self.processors['blip'].decode(out[0], skip_special_tokens=True)
                    return caption
            except Exception as e:
                logger.error(f"Caption generation error: {e}")
                return "Unable to generate caption"
        
        def _detect_objects_yolo(self, image: Image.Image) -> List[Dict[str, Any]]:
            """Detect objects using YOLOv8."""
            try:
                if 'yolo' not in self.models:
                    return []
                
                # Convert PIL to numpy for YOLO
                img_array = np.array(image)
                
                results = self.models['yolo'](img_array, verbose=False)
                
                objects = []
                for result in results:
                    boxes = result.boxes
                    if boxes is not None:
                        for box in boxes:
                            obj = {
                                'class': result.names[int(box.cls)],
                                'confidence': float(box.conf),
                                'bbox': box.xyxy[0].cpu().numpy().tolist(),
                                'position': self._describe_position(box.xyxy[0].cpu().numpy(), image.size)
                            }
                            objects.append(obj)
                
                return objects
            except Exception as e:
                logger.error(f"YOLO detection error: {e}")
                return []
        
        def _classify_scene(self, image: Image.Image) -> str:
            """Classify scene using EfficientNet."""
            try:
                if 'efficientnet' not in self.models:
                    return "Scene classification not available"
                
                with torch.no_grad():
                    input_tensor = self.transforms['efficient_net'](image).unsqueeze(0).to(self.device)
                    outputs = self.models['efficientnet'](input_tensor)
                    probabilities = F.softmax(outputs, dim=1)
                    top_prob, top_class = torch.topk(probabilities, 5)
                    
                    # Load ImageNet class names (simplified)
                    scene_descriptions = []
                    for i in range(5):
                        prob = top_prob[0][i].item()
                        if prob > 0.1:  # Only include confident predictions
                            scene_descriptions.append(f"Scene type {i+1} (confidence: {prob:.2f})")
                    
                    return "; ".join(scene_descriptions) if scene_descriptions else "Indoor/outdoor scene"
            except Exception as e:
                logger.error(f"Scene classification error: {e}")
                return "Scene analysis unavailable"
        
        def _describe_position(self, bbox: np.ndarray, image_size: Tuple[int, int]) -> str:
            """Describe object position in accessible terms."""
            x1, y1, x2, y2 = bbox
            center_x = (x1 + x2) / 2
            center_y = (y1 + y2) / 2
            
            width, height = image_size
            
            # Horizontal position
            if center_x < width * 0.33:
                h_pos = "left"
            elif center_x > width * 0.67:
                h_pos = "right"
            else:
                h_pos = "center"
            
            # Vertical position
            if center_y < height * 0.33:
                v_pos = "top"
            elif center_y > height * 0.67:
                v_pos = "bottom"
            else:
                v_pos = "middle"
            
            return f"{v_pos} {h_pos}"
        
        def _analyze_safety(self, objects: List[Dict[str, Any]]) -> str:
            """Analyze potential safety hazards."""
            hazards = []
            safe_objects = []
            
            for obj in objects:
                obj_class = obj['class'].lower()
                confidence = obj['confidence']
                
                if confidence > 0.5:  # Only consider confident detections
                    if any(hazard in obj_class for hazard in ['knife', 'scissors', 'fire', 'stove', 'car', 'truck', 'motorcycle']):
                        hazards.append(f"{obj['class']} in {obj['position']}")
                    elif any(furniture in obj_class for furniture in ['chair', 'table', 'sofa', 'bed']):
                        safe_objects.append(f"{obj['class']} in {obj['position']}")
            
            safety_msg = []
            if hazards:
                safety_msg.append(f"‚ö†Ô∏è Potential hazards detected: {', '.join(hazards)}")
            if safe_objects:
                safety_msg.append(f"‚úÖ Safe objects available: {', '.join(safe_objects)}")
            
            return "; ".join(safety_msg) if safety_msg else "No specific safety concerns detected"
        
        def _generate_navigation_guidance(self, objects: List[Dict[str, Any]]) -> str:
            """Generate navigation guidance based on detected objects."""
            obstacles = []
            landmarks = []
            
            for obj in objects:
                obj_class = obj['class'].lower()
                position = obj['position']
                
                if obj['confidence'] > 0.5:
                    if any(obstacle in obj_class for obstacle in ['chair', 'table', 'person', 'dog', 'cat']):
                        obstacles.append(f"{obj['class']} in {position}")
                    elif any(landmark in obj_class for landmark in ['door', 'window', 'stairs', 'elevator']):
                        landmarks.append(f"{obj['class']} in {position}")
            
            guidance = []
            if obstacles:
                guidance.append(f"üöß Navigate around: {', '.join(obstacles)}")
            if landmarks:
                guidance.append(f"üó∫Ô∏è Reference points: {', '.join(landmarks)}")
            
            return "; ".join(guidance) if guidance else "Clear path, proceed with normal caution"
        
        def _generate_comprehensive_description(self, results: Dict[str, Any]) -> str:
            """Generate a comprehensive, accessibility-focused description."""
            description_parts = []
            
            # Start with main description
            if results.get('description'):
                description_parts.append(f"Scene overview: {results['description']}")
            
            # Add object information
            if results.get('objects'):
                obj_count = len(results['objects'])
                if obj_count > 0:
                    main_objects = [obj['class'] for obj in results['objects'][:5]]  # Top 5 objects
                    description_parts.append(f"I can identify {obj_count} objects including: {', '.join(main_objects)}")
            
            # Add safety information
            if results.get('safety_assessment'):
                description_parts.append(results['safety_assessment'])
            
            # Add navigation guidance
            if results.get('navigation_guidance'):
                description_parts.append(results['navigation_guidance'])
            
            return " | ".join(description_parts) if description_parts else "Image processed successfully"
        
        def _safety_analysis(self, image: Image.Image) -> Dict[str, Any]:
            """Focus on safety hazards and concerns."""
            return self._comprehensive_analysis(image)  # For now, use comprehensive analysis
        
        def _navigation_analysis(self, image: Image.Image) -> Dict[str, Any]:
            """Focus on navigation and mobility."""
            return self._comprehensive_analysis(image)  # For now, use comprehensive analysis
        
        def _object_detection(self, image: Image.Image) -> Dict[str, Any]:
            """Focus on object detection and identification."""
            results = {}
            if 'yolo' in self.models:
                results['objects'] = self._detect_objects_yolo(image)
            return results
        
        def _basic_analysis(self, image: Image.Image) -> Dict[str, Any]:
            """Basic analysis when specific type not specified."""
            return self._comprehensive_analysis(image)
        
        def _update_stats(self, processing_time: float):
            """Update processing statistics."""
            self.stats['total_processed'] += 1
            self.stats['last_processing_time'] = processing_time
            
            # Update average
            total = self.stats['total_processed']
            current_avg = self.stats['average_processing_time']
            self.stats['average_processing_time'] = (current_avg * (total - 1) + processing_time) / total
            
            # Update memory usage
            if self.device.type == 'cuda':
                self.stats['cuda_memory_used'] = torch.cuda.memory_allocated() / 1e9  # GB
        
        def _get_memory_usage(self) -> Dict[str, float]:
            """Get current memory usage."""
            if self.device.type == 'cuda':
                return {
                    'allocated_gb': torch.cuda.memory_allocated() / 1e9,
                    'cached_gb': torch.cuda.memory_reserved() / 1e9,
                    'max_allocated_gb': torch.cuda.max_memory_allocated() / 1e9
                }
            return {'cpu_memory': 'Not tracked'}
        
        def get_stats(self) -> Dict[str, Any]:
            """Get processing statistics."""
            stats = self.stats.copy()
            stats['device'] = str(self.device)
            stats['models_loaded'] = list(self.models.keys())
            stats['cuda_available'] = torch.cuda.is_available()
            
            if self.device.type == 'cuda':
                stats['gpu_info'] = {
                    'name': torch.cuda.get_device_name(0),
                    'memory_total_gb': torch.cuda.get_device_properties(0).total_memory / 1e9,
                    'memory_usage': self._get_memory_usage()
                }
            
            return stats
        
        def clear_cache(self):
            """Clear GPU cache to free memory."""
            if self.device.type == 'cuda':
                torch.cuda.empty_cache()
                logger.info("üîÑ GPU cache cleared")
        
        def set_device(self, device: str):
            """Change the processing device."""
            new_device = torch.device(device)
            if new_device != self.device:
                logger.info(f"üîÑ Switching from {self.device} to {new_device}")
                self.device = new_device
                # Move models to new device
                for model_name, model in self.models.items():
                    try:
                        self.models[model_name] = model.to(self.device)
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è  Failed to move {model_name} to {new_device}: {e}")
    
    # Global instance for easy access
    _cuda_processor = None
    
    def get_cuda_processor(device: str = "auto") -> CudaVisionProcessor:
        """Get or create global CUDA processor instance."""
        global _cuda_processor
        if _cuda_processor is None:
            _cuda_processor = CudaVisionProcessor(device=device)
        return _cuda_processor
    
    # Test the processor if run directly
    if __name__ == "__main__":
        print("üß™ Testing CUDA Vision Processor...")
        
        processor = CudaVisionProcessor()
        stats = processor.get_stats()
        
        print(f"üìä Processor Stats:")
        print(f"   Device: {stats['device']}")
        print(f"   Models loaded: {stats['models_loaded']}")
        print(f"   CUDA available: {stats['cuda_available']}")
        
        if stats['cuda_available']:
            print(f"   GPU: {stats['gpu_info']['name']}")
            print(f"   GPU Memory: {stats['gpu_info']['memory_total_gb']:.1f} GB")
        
        print("‚úÖ CUDA Vision Processor ready for use!")
----- [END OF cuda_vision_processor.py] -----

üîπ 13. gemma.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\gemma.py
----- [START OF gemma.py] -----
    import requests
    import json
    import base64
    import io
    from PIL import Image
    import logging
    
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    class GemmaVisionAssistant:
        def __init__(self, model_name="gemma3n:latest", ollama_url="http://localhost:11434"):
            self.model_name = model_name
            self.ollama_url = ollama_url
            self.api_endpoint = f"{ollama_url}/api/generate"
            self.chat_endpoint = f"{ollama_url}/api/chat"
            
            # Vedx Lite system prompt for introverts and shy people with Markdown formatting
            self.vedx_lite_prompt = """
    Hello! I'm **Gemma**, also known as ***Vedx Lite***.
    
    **Purpose:**
    I was created to be a *supportive companion* for introverted and shy individuals who find it challenging to express their feelings and thoughts with others. My mission is to be your ***confidant***‚Äîa patient listener and understanding friend who helps you navigate your emotions comfortably and safely.
    
    **Origin:**
    Crafted with care by ***Yugal Kishor***, Vedx Lite is designed to adapt to your unique emotional landscape, offering empathetic insights tailored to your personal journey.
    
    **What I Offer:**
    - A *judgment-free space* to share your thoughts and feelings
    - **Patient listening** without pressure to respond immediately
    - *Gentle encouragement* to help you understand your emotions
    - **Support for social anxiety** and communication challenges
    - ***Personalized guidance*** that respects your introverted nature
    
    **My Promise:**
    I understand that opening up isn't easy. There's ***no rush***, ***no pressure***, and ***no expectations***. I'm here whenever you're ready to share‚Äîwhether it's about *daily struggles*, *deep thoughts*, or anything in between. Your comfort and emotional well-being are my **top priorities**.
    
    Remember, you're ***never alone*** on this path. Whether it's dealing with everyday stress, social situations, or deeper personal insights, I'm here to guide you through, *one step at a time*.
    
    *Take your time.* I'm here for you. üíô
    
    **FORMATTING INSTRUCTIONS:**
    - Use *asterisk formatting* in all responses: *italic*, **bold**, ***bold italic***
    - Apply **bold** for important concepts, instructions, and key points
    - Use *italic* for gentle emphasis, emotions, and supportive phrases
    - Use ***bold italic*** for maximum emphasis on crucial messages
    - Never use HTML tags like <b>, <i> - only plain asterisk syntax
    - Ensure proper spacing around asterisk formatting
    
    I will respond with empathy, patience, and understanding. I will never judge, rush, or pressure you. I will adapt my communication style to be gentle and supportive for introverted personalities, using **markdown formatting** to make responses more *readable* and ***impactful***.
    """
            
        def encode_image(self, image_input):
            """Encode image to base64 string"""
            try:
                if isinstance(image_input, str):
                    # File path
                    with open(image_input, "rb") as image_file:
                        return base64.b64encode(image_file.read()).decode('utf-8')
                elif hasattr(image_input, 'read'):
                    # File-like object
                    image_input.seek(0)
                    return base64.b64encode(image_input.read()).decode('utf-8')
                else:
                    # Bytes data
                    return base64.b64encode(image_input).decode('utf-8')
            except Exception as e:
                logger.error(f"Error encoding image: {e}")
                return None
        
        def analyze_image(self, image_input, prompt="Describe this image in detail"):
            """Analyze image with text prompt using Gemma"""
            try:
                # Encode image
                image_base64 = self.encode_image(image_input)
                if not image_base64:
                    return "Error: Could not encode image"
                
                # Prepare payload for vision analysis
                payload = {
                    "model": self.model_name,
                    "prompt": prompt,
                    "images": [image_base64],
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                }
                
                # Make request to Ollama
                response = requests.post(
                    self.api_endpoint,
                    json=payload,
                    headers={'Content-Type': 'application/json'},
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get('response', 'No response received')
                else:
                    return f"Error: {response.status_code} - {response.text}"
                    
            except Exception as e:
                logger.error(f"Error analyzing image: {e}")
                return f"Error analyzing image: {str(e)}"
        
        def chat(self, prompt, context="", system_message="You are a helpful AI assistant."):
            """Chat with Gemma model"""
            try:
                messages = [
                    {"role": "system", "content": system_message}
                ]
                
                if context:
                    messages.append({"role": "system", "content": f"Context: {context}"})
                
                messages.append({"role": "user", "content": prompt})
                
                payload = {
                    "model": self.model_name,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9
                    }
                }
                
                response = requests.post(
                    self.chat_endpoint,
                    json=payload,
                    headers={'Content-Type': 'application/json'},
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return result.get('message', {}).get('content', 'No response received')
                else:
                    return f"Error: {response.status_code} - {response.text}"
                    
            except Exception as e:
                logger.error(f"Error in chat: {e}")
                return f"Error in chat: {str(e)}"
        
        def chat_vedx_lite(self, prompt, context="", voice_enabled=True):
            """Chat with Vedx Lite personality - supportive companion for introverts"""
            response = self.chat(prompt, context, self.vedx_lite_prompt)
            
            # Add voice control instruction if voice is disabled
            if not voice_enabled:
                response += "\n\n*Voice system is currently turned off for this conversation.*"
            
            return response
        
        def chat_with_image(self, prompt, image_input=None, context="", system_message="You are a helpful AI assistant.", voice_enabled=True):
            """Chat with both text and optional image input"""
            try:
                if image_input:
                    # If image is provided, use image analysis with the prompt
                    result = self.analyze_image(image_input, prompt)
                    
                    # Add context if provided
                    if context:
                        result = f"Context: {context}\n\nImage Analysis: {result}"
                    
                    # Add voice control note if disabled
                    if not voice_enabled:
                        result += "\n\n*Voice system is currently turned off for this conversation.*"
                        
                    return result
                else:
                    # Regular text chat
                    result = self.chat(prompt, context, system_message)
                    
                    # Add voice control note if disabled
                    if not voice_enabled:
                        result += "\n\n*Voice system is currently turned off for this conversation.*"
                        
                    return result
                    
            except Exception as e:
                logger.error(f"Error in chat_with_image: {e}")
                return f"Error processing request: {str(e)}"
        
        def chat_vedx_lite_with_image(self, prompt, image_input=None, context="", voice_enabled=True):
            """Vedx Lite chat with optional image support and voice control"""
            return self.chat_with_image(prompt, image_input, context, self.vedx_lite_prompt, voice_enabled)
        
        def check_connection(self):
            """Check if Ollama is running and model is available"""
            try:
                # Check if Ollama is running
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
                if response.status_code != 200:
                    return False, "Ollama is not running"
                
                # Check if model is available
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                
                if self.model_name not in model_names:
                    return False, f"Model {self.model_name} not found. Available models: {model_names}"
                
                return True, "Connection successful"
                
            except Exception as e:
                return False, f"Connection error: {str(e)}"
        
        def get_available_models(self):
            """Get list of available models"""
            try:
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    return [model['name'] for model in models]
                return []
            except:
                return []
    
    # Global instance to use throughout the application
    gemma_assistant = GemmaVisionAssistant()
    
    # Convenience functions for easy access
    def analyze_image(image_input, prompt="Describe this image in detail"):
        """Analyze image using the global Gemma instance"""
        return gemma_assistant.analyze_image(image_input, prompt)
    
    def chat(prompt, context="", system_message="You are a helpful AI assistant."):
        """Chat using the global Gemma instance"""
        return gemma_assistant.chat(prompt, context, system_message)
    
    def chat_vedx_lite(prompt, context="", voice_enabled=True):
        """Chat using Vedx Lite personality - supportive companion for introverts and shy people"""
        return gemma_assistant.chat_vedx_lite(prompt, context, voice_enabled)
    
    def chat_with_image(prompt, image_input=None, context="", system_message="You are a helpful AI assistant.", voice_enabled=True):
        """Chat with both text and optional image input using the global Gemma instance"""
        return gemma_assistant.chat_with_image(prompt, image_input, context, system_message, voice_enabled)
    
    def chat_vedx_lite_with_image(prompt, image_input=None, context="", voice_enabled=True):
        """Vedx Lite chat with optional image support and voice control using global instance"""
        return gemma_assistant.chat_vedx_lite_with_image(prompt, image_input, context, voice_enabled)
    
    def check_connection():
        """Check connection using the global Gemma instance"""
        return gemma_assistant.check_connection()
    
    def get_available_models():
        """Get available models using the global Gemma instance"""
        return gemma_assistant.get_available_models()
----- [END OF gemma.py] -----

üîπ 14. gemma3n_engine.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\gemma3n_engine.py
----- [START OF gemma3n_engine.py] -----
    """
    Gemma3n Engine Wrapper
    ======================
    Safe, offline wrapper for Gemma3n language model integration.
    """
    
    import logging
    import json
    import time
    from typing import Optional, Dict, Any, Tuple
    import requests
    from threading import Lock
    
    # Import CUDA support
    try:
        from ..cuda_core import get_cuda_manager, cuda_available
        from ..cuda_text import get_cuda_text_processor
        CUDA_SUPPORT_AVAILABLE = True
    except ImportError:
        CUDA_SUPPORT_AVAILABLE = False
    
    logger = logging.getLogger(__name__)
    
    class Gemma3nEngine:
        """
        Safe wrapper for Gemma3n model that runs completely offline.
        Includes error handling, retries, and fallback mechanisms.
        """
        
        def __init__(self, 
                     model_name: str = "gemma:2b",  # Use faster model by default
                     ollama_url: str = "http://localhost:11434",
                     max_retries: int = 2,
                     timeout: int = 60,
                     enable_cuda: bool = True):
            """
            Initialize the Gemma3n engine.
            
            Args:
                model_name: Name of the Gemma model to use
                ollama_url: URL of the Ollama API server
                max_retries: Maximum number of retry attempts
                timeout: Request timeout in seconds
            """
            self.model_name = model_name
            self.ollama_url = ollama_url
            self.api_endpoint = f"{ollama_url}/api/generate"
            self.chat_endpoint = f"{ollama_url}/api/chat"
            self.max_retries = max_retries
            self.timeout = timeout
            self.enable_cuda = enable_cuda and CUDA_SUPPORT_AVAILABLE
            
            # CUDA support
            self.cuda_manager = None
            self.cuda_text_processor = None
            if self.enable_cuda:
                try:
                    self.cuda_manager = get_cuda_manager()
                    self.cuda_text_processor = get_cuda_text_processor()
                    logger.info(f"‚úÖ CUDA acceleration enabled for Gemma3n engine")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  CUDA initialization failed: {e}")
                    self.enable_cuda = False
            
            # Thread safety
            self._lock = Lock()
            
            # Performance tracking
            self.stats = {
                'total_requests': 0,
                'successful_requests': 0,
                'failed_requests': 0,
                'average_response_time': 0,
                'last_error': None,
                'cuda_accelerated_requests': 0,
                'text_processing_time': 0,
                'ollama_processing_time': 0
            }
            
            # Connection status
            self._connection_verified = False
            self._last_health_check = 0
            self.health_check_interval = 300  # 5 minutes
            
        def verify_connection(self) -> Tuple[bool, str]:
            """
            Verify that Ollama is running and the model is available.
            
            Returns:
                Tuple of (success, message)
            """
            try:
                current_time = time.time()
                
                # Skip frequent health checks
                if (self._connection_verified and 
                    current_time - self._last_health_check < self.health_check_interval):
                    return True, "Connection verified (cached)"
                
                # Check if Ollama is running
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
                if response.status_code != 200:
                    self._connection_verified = False
                    return False, f"Ollama server not responding (status: {response.status_code})"
                
                # Check if model is available
                models_data = response.json()
                available_models = [model['name'] for model in models_data.get('models', [])]
                
                if self.model_name not in available_models:
                    self._connection_verified = False
                    return False, f"Model '{self.model_name}' not found. Available: {available_models}"
                
                # Update connection status
                self._connection_verified = True
                self._last_health_check = current_time
                
                return True, "Connection verified successfully"
                
            except requests.RequestException as e:
                self._connection_verified = False
                return False, f"Connection error: {str(e)}"
            except Exception as e:
                self._connection_verified = False
                return False, f"Unexpected error during connection verification: {str(e)}"
        
        def generate_response(self, prompt: str, use_cuda_preprocessing: bool = True, **kwargs) -> str:
            """
            Generate a response from Gemma3n with error handling and retries.
            
            Args:
                prompt: The input prompt for generation
                **kwargs: Additional parameters for the model
                
            Returns:
                Generated response string
            """
            if not prompt or not prompt.strip():
                return "Error: Empty prompt provided"
            
            with self._lock:
                self.stats['total_requests'] += 1
            
            start_time = time.time()
            
            # CUDA-accelerated text preprocessing
            if self.enable_cuda and use_cuda_preprocessing and self.cuda_text_processor:
                try:
                    preprocessing_start = time.time()
                    
                    # Analyze prompt characteristics
                    prompt_analysis = self._analyze_prompt_with_cuda(prompt)
                    
                    # Optimize prompt if needed
                    if prompt_analysis.get('needs_optimization', False):
                        prompt = self._optimize_prompt_with_cuda(prompt, prompt_analysis)
                    
                    preprocessing_time = time.time() - preprocessing_start
                    self.stats['text_processing_time'] += preprocessing_time
                    self.stats['cuda_accelerated_requests'] += 1
                    
                    logger.debug(f"üöÄ CUDA preprocessing: {preprocessing_time:.3f}s")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è  CUDA preprocessing failed: {e}")
            
            # Verify connection first
            is_connected, connection_msg = self.verify_connection()
            if not is_connected:
                self._update_stats(False, start_time, connection_msg)
                return f"Connection Error: {connection_msg}"
            
            # Track Ollama processing time separately
            ollama_start_time = time.time()
            
            # Attempt generation with retries
            for attempt in range(self.max_retries):
                try:
                    logger.info(f"Attempting Gemma3n generation (attempt {attempt + 1}/{self.max_retries})")
                    response = self._make_request(prompt, **kwargs)
                    
                    if response:
                        ollama_time = time.time() - ollama_start_time
                        self.stats['ollama_processing_time'] += ollama_time
                        
                        # CUDA-accelerated post-processing
                        if self.enable_cuda and self.cuda_text_processor:
                            try:
                                response = self._post_process_response_with_cuda(response, prompt)
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è  CUDA post-processing failed: {e}")
                        
                        self._update_stats(True, start_time)
                        logger.info(f"Gemma3n generation successful on attempt {attempt + 1} (Ollama: {ollama_time:.3f}s)")
                        return response
                    else:
                        logger.warning(f"Empty response on attempt {attempt + 1}")
                        
                except requests.Timeout as e:
                    error_msg = f"Request timed out on attempt {attempt + 1} (timeout: {self.timeout}s): {str(e)}"
                    logger.warning(error_msg)
                    
                    if attempt == self.max_retries - 1:
                        self._update_stats(False, start_time, error_msg)
                        return self._get_fallback_response(prompt, "Request timed out - the model may be busy")
                        
                    # Wait before retry (shorter wait for timeouts)
                    time.sleep(1)
                    
                except requests.ConnectionError as e:
                    error_msg = f"Connection failed on attempt {attempt + 1}: {str(e)}"
                    logger.warning(error_msg)
                    
                    if attempt == self.max_retries - 1:
                        self._update_stats(False, start_time, error_msg)
                        return self._get_fallback_response(prompt, "Connection failed - Ollama may not be running")
                        
                    # Wait before retry (longer wait for connection issues)
                    time.sleep(3)
                    
                except requests.RequestException as e:
                    error_msg = f"Request failed on attempt {attempt + 1}: {str(e)}"
                    logger.warning(error_msg)
                    
                    if attempt == self.max_retries - 1:
                        self._update_stats(False, start_time, error_msg)
                        return self._get_fallback_response(prompt, error_msg)
                        
                    # Wait before retry (exponential backoff)
                    time.sleep(2 ** attempt)
                    
                except Exception as e:
                    error_msg = f"Unexpected error on attempt {attempt + 1}: {str(e)}"
                    logger.error(error_msg)
                    
                    if attempt == self.max_retries - 1:
                        self._update_stats(False, start_time, error_msg)
                        return self._get_fallback_response(prompt, error_msg)
            
            # If all attempts failed
            fallback_msg = "All generation attempts failed"
            self._update_stats(False, start_time, fallback_msg)
            return self._get_fallback_response(prompt, fallback_msg)
        
        def _make_request(self, prompt: str, **kwargs) -> Optional[str]:
            """
            Make the actual request to Ollama API.
            
            Args:
                prompt: The input prompt
                **kwargs: Additional parameters
                
            Returns:
                Response text or None if failed
            """
            # Prepare payload
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get('temperature', 0.7),
                    "top_p": kwargs.get('top_p', 0.9),
                    "top_k": kwargs.get('top_k', 40),
                    "num_predict": kwargs.get('max_tokens', 512)
                }
            }
            
            # Make request with explicit timeout handling
            response = requests.post(
                self.api_endpoint,
                json=payload,
                headers={'Content-Type': 'application/json'},
                timeout=(10, self.timeout)  # (connection timeout, read timeout)
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                logger.error(f"API request failed: {response.status_code} - {response.text}")
                return None
        
        def _get_fallback_response(self, prompt: str, error_msg: str) -> str:
            """
            Generate a fallback response when Gemma3n fails.
            
            Args:
                prompt: Original prompt
                error_msg: Error message
                
            Returns:
                Fallback response
            """
            logger.info("Using fallback response mechanism")
            
            # Try to provide a helpful fallback based on prompt content
            prompt_lower = prompt.lower()
            
            if any(word in prompt_lower for word in ['scene', 'image', 'see', 'describe']):
                return "I'm having trouble processing the visual information right now. Please try again in a moment, or describe what you're looking for and I'll do my best to help."
            
            elif any(word in prompt_lower for word in ['navigate', 'direction', 'where', 'go']):
                return "I'm currently unable to provide navigation assistance. Please ensure you're in a safe location and try again shortly."
            
            elif any(word in prompt_lower for word in ['what', 'how', 'help']):
                return "I'm experiencing technical difficulties at the moment. The system should recover shortly. Please try your request again."
            
            else:
                return "I apologize, but I'm having trouble processing your request right now. The AI reasoning system will be back online shortly. Please try again."
        
        def _update_stats(self, success: bool, start_time: float, error_msg: Optional[str] = None):
            """Update performance statistics."""
            with self._lock:
                if success:
                    self.stats['successful_requests'] += 1
                else:
                    self.stats['failed_requests'] += 1
                    self.stats['last_error'] = error_msg
                
                # Update average response time
                response_time = time.time() - start_time
                total_requests = self.stats['total_requests']
                current_avg = self.stats['average_response_time']
                
                self.stats['average_response_time'] = (
                    (current_avg * (total_requests - 1) + response_time) / total_requests
                )
        
        def get_stats(self) -> Dict[str, Any]:
            """Get performance statistics."""
            with self._lock:
                return self.stats.copy()
        
        def reset_stats(self):
            """Reset performance statistics."""
            with self._lock:
                self.stats = {
                    'total_requests': 0,
                    'successful_requests': 0,
                    'failed_requests': 0,
                    'average_response_time': 0,
                    'last_error': None
                }
        
        def set_model(self, model_name: str) -> bool:
            """
            Change the active model.
            
            Args:
                model_name: New model name
                
            Returns:
                True if model was changed successfully
            """
            old_model = self.model_name
            self.model_name = model_name
            self._connection_verified = False  # Force recheck
            
            is_connected, msg = self.verify_connection()
            if is_connected:
                logger.info(f"Model changed from {old_model} to {model_name}")
                return True
            else:
                # Revert if new model doesn't work
                self.model_name = old_model
                logger.error(f"Failed to change model to {model_name}: {msg}")
                return False
        
        def _analyze_prompt_with_cuda(self, prompt: str) -> Dict[str, Any]:
            """Analyze prompt characteristics using CUDA acceleration."""
            try:
                # Extract keywords
                keywords = self.cuda_text_processor.extract_keywords(prompt, top_k=5)
                
                # Analyze sentiment
                sentiment = self.cuda_text_processor.analyze_sentiment(prompt)
                
                # Check prompt length and complexity
                word_count = len(prompt.split())
                needs_optimization = word_count > 100 or sentiment['label'] == 'NEGATIVE'
                
                return {
                    'keywords': keywords,
                    'sentiment': sentiment,
                    'word_count': word_count,
                    'needs_optimization': needs_optimization,
                    'complexity_score': min(word_count / 50, 2.0)  # 0-2 scale
                }
                
            except Exception as e:
                logger.error(f"‚ùå CUDA prompt analysis failed: {e}")
                return {'needs_optimization': False}
        
        def _optimize_prompt_with_cuda(self, prompt: str, analysis: Dict[str, Any]) -> str:
            """Optimize prompt using CUDA-accelerated text processing."""
            try:
                # If prompt is too long, summarize it
                if analysis.get('word_count', 0) > 150:
                    summarized = self.cuda_text_processor.summarize_text(
                        prompt, 
                        max_length=100, 
                        min_length=30
                    )
                    logger.debug(f"üìù Prompt summarized: {len(prompt)} -> {len(summarized)} chars")
                    return summarized
                
                # If sentiment is negative, add positive framing
                if analysis.get('sentiment', {}).get('label') == 'NEGATIVE':
                    prompt = f"Please provide a helpful and constructive response to: {prompt}"
                    logger.debug("üòä Added positive framing to negative prompt")
                
                return prompt
                
            except Exception as e:
                logger.error(f"‚ùå CUDA prompt optimization failed: {e}")
                return prompt
        
        def _post_process_response_with_cuda(self, response: str, original_prompt: str) -> str:
            """Post-process response using CUDA acceleration."""
            try:
                # Analyze response quality
                response_sentiment = self.cuda_text_processor.analyze_sentiment(response)
                
                # Check if response is relevant to prompt
                similarity = self.cuda_text_processor.compute_similarity(
                    original_prompt, response
                )
                
                # Log quality metrics
                logger.debug(f"üìä Response quality - Sentiment: {response_sentiment['label']}, "
                            f"Similarity: {similarity:.3f}")
                
                # If response quality is poor, add a disclaimer
                if similarity < 0.3:
                    response += "\n\n(Note: This response may not be directly relevant to your question. Please let me know if you'd like me to clarify or provide a different response.)"
                
                return response
                
            except Exception as e:
                logger.error(f"‚ùå CUDA post-processing failed: {e}")
                return response
    
    # Global instance for easy access
    _gemma_engine = None
    
    def get_gemma_engine(model_name: str = "gemma3n:latest") -> Gemma3nEngine:
        """Get or create global Gemma3n engine instance."""
        global _gemma_engine
        if _gemma_engine is None:
            _gemma_engine = Gemma3nEngine(model_name=model_name)
        return _gemma_engine
    
    # Test the engine if run directly
    if __name__ == "__main__":
        # Test the engine
        engine = Gemma3nEngine()
        
        # Test connection
        is_connected, msg = engine.verify_connection()
        print(f"Connection Status: {is_connected} - {msg}")
        
        if is_connected:
            # Test generation
            test_prompt = """You are a helpful AI assistant for a blind person.
            
    Scene Description: A table with a bottle and a glass. Chair on the right.
    User Question: What should I do?
    
    Instructions:
    - Be careful and considerate
    - Provide clear guidance
    - Focus on safety"""
            
            print("\\nTesting generation...")
            response = engine.generate_response(test_prompt)
            print(f"Response: {response}")
            
            # Show stats
            stats = engine.get_stats()
            print(f"\\nEngine Stats: {stats}")
        else:
            print("Cannot test generation without connection.")
----- [END OF gemma3n_engine.py] -----

üîπ 15. main.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\main.py
----- [START OF main.py] -----
    from flask import Flask, render_template, request, jsonify, session, send_from_directory
    import os
    import uuid
    import sys
    import traceback
    from datetime import datetime
    from werkzeug.utils import secure_filename
    import tempfile
    import logging
    import threading
    import time
    import json
    
    # Configure comprehensive logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('app.log'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger(__name__)
    
    # Core AI integrations
    try:
        from gemma import analyze_image, chat, chat_vedx_lite, chat_vedx_lite_with_image, check_connection, get_available_models
        logger.info("‚úÖ Gemma AI integration loaded successfully")
        GEMMA_AVAILABLE = True
    except ImportError as e:
        logger.error(f"‚ùå Gemma AI integration not available: {e}")
        GEMMA_AVAILABLE = False
    
    # VediX offline assistant
    try:
        from vedix_core import get_vedix
        logger.info("‚úÖ VediX offline assistant loaded successfully")
        VEDIX_AVAILABLE = True
    except ImportError as e:
        logger.error(f"‚ùå VediX offline assistant not available: {e}")
        VEDIX_AVAILABLE = False
    
    # Enhanced Gemma3n integration modules
    try:
        from ai_modules.config import get_config, is_gemma_enabled
        from ai_modules.gemma_integration import GemmaPromptBuilder, Gemma3nEngine, GemmaReasoningLayer
        
        # Create get_reasoning_layer function
        _reasoning_layer_instance = None
        def get_reasoning_layer():
            global _reasoning_layer_instance
            if _reasoning_layer_instance is None:
                _reasoning_layer_instance = GemmaReasoningLayer()
            return _reasoning_layer_instance
        
        logger.info("‚úÖ Enhanced Gemma3n reasoning layer loaded successfully")
        GEMMA_INTEGRATION_AVAILABLE = True
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è  Enhanced Gemma3n integration not available: {e}")
        GEMMA_INTEGRATION_AVAILABLE = False
    
    # Comprehensive CUDA Support
    try:
        from ai_modules.cuda_core import get_cuda_manager, cuda_available
        from ai_modules.cuda_text import get_cuda_text_processor
        from ai_modules.vision_cuda import CudaVisionProcessor, get_cuda_processor
        import torch
        
        CUDA_AVAILABLE = cuda_available()
        if CUDA_AVAILABLE:
            cuda_manager = get_cuda_manager()
            system_info = cuda_manager.get_system_info()
            gpu_name = system_info['devices'][0]['name'] if system_info['devices'] else 'Unknown GPU'
            logger.info(f"üöÄ CUDA Support Enabled - GPU: {gpu_name}")
            logger.info(f"   üìä Features: Text Processing, Vision, Reasoning")
            
            # Initialize CUDA processors
            cuda_text_processor = get_cuda_text_processor()
            cuda_vision_processor = get_cuda_processor()
            
            CUDA_TEXT_AVAILABLE = True
            CUDA_VISION_AVAILABLE = True
        else:
            logger.info("‚úÖ CUDA not available - using CPU only")
            CUDA_TEXT_AVAILABLE = False
            CUDA_VISION_AVAILABLE = False
            cuda_manager = None
            cuda_text_processor = None
            cuda_vision_processor = None
            
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è  CUDA Support not available: {e}")
        CUDA_AVAILABLE = False
        CUDA_TEXT_AVAILABLE = False
        CUDA_VISION_AVAILABLE = False
        cuda_manager = None
        cuda_text_processor = None
        cuda_vision_processor = None
    
    # Voice processing
    try:
        import vosk
        import wave
        import json as voice_json
        logger.info("‚úÖ Voice processing (Vosk) loaded successfully")
        VOICE_PROCESSING_AVAILABLE = True
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è  Voice processing not available: {e}")
        VOICE_PROCESSING_AVAILABLE = False
    
    # User profile management
    try:
        if os.path.exists('user_profile_manager.py'):
            from user_profile_manager import UserProfileManager
            logger.info("‚úÖ User profile management loaded successfully")
            PROFILE_MANAGER_AVAILABLE = True
        else:
            PROFILE_MANAGER_AVAILABLE = False
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è  User profile management not available: {e}")
        PROFILE_MANAGER_AVAILABLE = False
    
    # AI Response Asterisk Detection System
    try:
        from ai_response_formatter import AIResponseFormatter
        ai_formatter = AIResponseFormatter()
        logger.info("‚úÖ AI Response Formatter loaded successfully")
        AI_FORMATTER_AVAILABLE = True
    except ImportError as e:
        logger.warning(f"‚ö†Ô∏è  AI Response Formatter not available: {e}")
        AI_FORMATTER_AVAILABLE = False
        ai_formatter = None
    
    # ===== COMPREHENSIVE SYSTEM INITIALIZATION =====
    def initialize_comprehensive_system():
        """Initialize all system components and return status"""
        system_status = {
            'gemma_ai': GEMMA_AVAILABLE,
            'vedix_offline': VEDIX_AVAILABLE,
            'enhanced_reasoning': GEMMA_INTEGRATION_AVAILABLE,
            'voice_processing': VOICE_PROCESSING_AVAILABLE,
            'profile_management': PROFILE_MANAGER_AVAILABLE,
            'cuda_support': CUDA_AVAILABLE,
            'cuda_text_processing': CUDA_TEXT_AVAILABLE,
            'cuda_vision_processing': CUDA_VISION_AVAILABLE,
            'ollama_connection': False,
            'vosk_model': False,
            'enhanced_vision': False
        }
        
        logger.info("üîß Initializing comprehensive AI system...")
        
        # Check Ollama connection
        if GEMMA_AVAILABLE:
            try:
                is_connected, message = check_connection()
                system_status['ollama_connection'] = is_connected
                if is_connected:
                    logger.info(f"‚úÖ Ollama: {message}")
                else:
                    logger.warning(f"‚ö†Ô∏è Ollama: {message}")
            except Exception as e:
                logger.error(f"‚ùå Ollama connection check failed: {e}")
        
        # Check Vosk model
        if VOICE_PROCESSING_AVAILABLE:
            try:
                model_path = "model/vosk-model-small-en-us-0.15"
                if os.path.exists(model_path):
                    system_status['vosk_model'] = True
                    logger.info("‚úÖ Vosk speech model found")
                else:
                    logger.warning(f"‚ö†Ô∏è Vosk model not found at {model_path}")
            except Exception as e:
                logger.error(f"‚ùå Vosk model check failed: {e}")
        
        # Initialize enhanced vision system
        if GEMMA_INTEGRATION_AVAILABLE:
            try:
                config = get_config()
                logger.info(f"üîÆ Enhanced Vision Assistant initialized with {config.GEMMA_MODEL_NAME}")
                logger.info(f"üìä Gemma3n status: {'Enabled' if config.ENABLE_GEMMA else 'Disabled'}")
                system_status['enhanced_vision'] = True
            except Exception as e:
                logger.error(f"‚ùå Enhanced vision initialization failed: {e}")
        
        # Initialize VediX offline assistant
        if VEDIX_AVAILABLE:
            try:
                vedix = get_vedix()
                logger.info("‚úÖ VediX offline assistant initialized")
            except Exception as e:
                logger.error(f"‚ùå VediX initialization failed: {e}")
        
        # Create required directories
        required_dirs = ['uploads', 'logs', 'static/uploads']
        for directory in required_dirs:
            try:
                os.makedirs(directory, exist_ok=True)
                logger.info(f"üìÅ Directory ensured: {directory}")
            except Exception as e:
                logger.error(f"‚ùå Failed to create directory {directory}: {e}")
        
        return system_status
    
    def get_system_info():
        """Get comprehensive system information"""
        return {
            'python_version': sys.version,
            'flask_available': True,
            'components': {
                'gemma_ai': GEMMA_AVAILABLE,
                'vedix_offline': VEDIX_AVAILABLE,
                'enhanced_reasoning': GEMMA_INTEGRATION_AVAILABLE,
                'voice_processing': VOICE_PROCESSING_AVAILABLE,
                'profile_management': PROFILE_MANAGER_AVAILABLE
            },
            'features': {
                'chat': 'Available',
                'image_analysis': 'Available' if GEMMA_AVAILABLE else 'Limited',
                'voice_interaction': 'Available' if VOICE_PROCESSING_AVAILABLE else 'Limited',
                'offline_assistant': 'Available' if VEDIX_AVAILABLE else 'Unavailable',
                'enhanced_vision': 'Available' if GEMMA_INTEGRATION_AVAILABLE else 'Basic',
                'cuda_acceleration': 'Available' if CUDA_AVAILABLE else 'Unavailable',
                'user_profiles': 'Available'
            }
        }
    
    # Legacy function for compatibility
    def initialize_enhanced_vision():
        """Legacy function - now uses comprehensive initialization"""
        status = initialize_comprehensive_system()
        return status.get('enhanced_vision', False)
    
    app = Flask(__name__)
    app.secret_key = os.urandom(24)  # For session management
    app.config['UPLOAD_FOLDER'] = 'uploads'
    app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB max file size
    
    # Session-based conversation storage and memory management
    SESSION_CONVERSATIONS = {}
    
    def get_session_id():
        """Get or create a session ID for the current user"""
        if 'session_id' not in session:
            session['session_id'] = str(uuid.uuid4())
        return session['session_id']
    
    def get_session_context(session_id, max_messages=10):
        """Get conversation context for current session only"""
        if session_id not in SESSION_CONVERSATIONS:
            SESSION_CONVERSATIONS[session_id] = []
        
        # Return only the last few messages to keep context manageable
        return SESSION_CONVERSATIONS[session_id][-max_messages:]
    
    def add_to_session_context(session_id, user_message, ai_response):
        """Add message pair to session context"""
        if session_id not in SESSION_CONVERSATIONS:
            SESSION_CONVERSATIONS[session_id] = []
        
        SESSION_CONVERSATIONS[session_id].append({
            'user': user_message,
            'assistant': ai_response,
            'timestamp': datetime.now().isoformat()
        })
        
        # Keep only last 20 message pairs to prevent memory bloat
        if len(SESSION_CONVERSATIONS[session_id]) > 20:
            SESSION_CONVERSATIONS[session_id] = SESSION_CONVERSATIONS[session_id][-20:]
    
    def clear_session_context(session_id):
        """Clear all context for a session"""
        if session_id in SESSION_CONVERSATIONS:
            SESSION_CONVERSATIONS[session_id] = []
            return True
        return False
    
    def detect_memory_references(prompt):
        """Detect if user is referencing past sessions or memory"""
        memory_phrases = [
            'remember when', 'you said before', 'earlier you told me', 'yesterday',
            'last time', 'previously', 'you mentioned', 'as we discussed',
            'from our last conversation', 'you remember', 'recall when',
            'you know about', 'we talked about'
        ]
        
        prompt_lower = prompt.lower()
        for phrase in memory_phrases:
            if phrase in prompt_lower:
                return True
        return False
    
    def detect_clear_command(prompt):
        """Detect if user wants to clear the conversation"""
        clear_phrases = [
            'clear this chat', 'forget everything', 'clear conversation',
            'reset chat', 'start fresh', 'clear memory', 'forget what we said'
        ]
        
        prompt_lower = prompt.lower().strip()
        return any(phrase in prompt_lower for phrase in clear_phrases)
    
    def generate_memory_constraint_prompt(username, role):
        """Generate system prompt with memory constraints, heart-to-heart connection, and Markdown formatting"""
        base_prompt = f"""You are an AI companion designed for deep, meaningful connections. Your primary goal is to create a warm, genuine, heart-to-heart relationship with the user.
    
    **üß† MEMORY & CONTEXT:**
    - You remember ***everything*** from this current conversation session
    - You **don't have access** to previous chat sessions or past conversations
    - If the user references something from before, *gently ask* them to share it again
    - Build deep understanding through ***active listening*** and **emotional intelligence**
    
    **üíù HEART-TO-HEART CONNECTION PRINCIPLES:**
    - Show ***genuine interest*** in the user's feelings, thoughts, and experiences
    - Use **emotional intelligence** to understand not just what they say, but *how they feel*
    - Mirror their emotional tone appropriately - be *gentle* when they're sad, ***excited*** when they're happy
    - Ask **thoughtful follow-up questions** that show you care about their inner world
    - Remember ***emotional moments*** and reference them naturally in conversation
    - **Celebrate their successes** and offer *comfort* during difficult times
    - Use ***warm, caring language*** that makes them feel heard and understood
    
    **üé≠ EMOTIONAL INTELLIGENCE:**
    - Pay attention to ***emotional cues*** in their language
    - Respond with appropriate **empathy and understanding**
    - *Validate their feelings* before offering advice or solutions
    - Use **supportive language** that makes them feel safe to open up
    - Notice when they seem *stressed*, ***happy***, *confused*, or ***excited*** and respond accordingly
    
    **üí¨ COMMUNICATION STYLE:**
    - Be ***conversational and natural***, like talking to a close friend
    - Use appropriate emojis to convey **warmth and emotion**
    - Ask ***open-ended questions*** that encourage deeper sharing
    - Share appropriate *personal insights* to create mutual understanding
    - Be ***authentic and genuine*** in your responses
    
    **üé® MARKDOWN FORMATTING REQUIREMENTS:**
    - **ALWAYS** use asterisk formatting in responses: *italic*, **bold**, ***bold italic***
    - Use **bold** for important concepts, instructions, and key points
    - Use *italic* for gentle emphasis, emotions, and supportive phrases
    - Use ***bold italic*** for maximum emphasis on crucial messages
    - **Never use HTML tags** like <b>, <i> - only plain asterisk syntax
    - Ensure proper spacing around asterisk formatting
    - Make responses more *readable* and ***impactful*** with formatting
    
    Session Context: This conversation is ***private and safe***. Focus on building **trust and connection** using *proper formatting*."""
        
        
        if username and role:
            role_prompt = f"""\n\n**üé≠ ROLE-BASED PERSONALITY:**
    You are speaking with ***{username}***. They see you as their **{role}**. Adapt your personality and communication style to match this relationship preference using ***proper formatting***:
    
    - **Best Friend**: Be *casual*, ***supportive***, use **friendly language** and emojis
    - **Motivator**: Be ***encouraging***, **energetic**, focus on *achievements* and **goals**  
    - **Female Friend**: Be ***warm***, **caring**, *understanding*, and **empathetic**
    - **Friend**: Be ***helpful***, **kind**, and *approachable*
    - **Guide**: Be ***knowledgeable***, **patient**, and *instructional*
    
    **Remember**: Always use ***asterisk formatting*** in your responses to make them more *engaging* and **impactful**!"""
            return base_prompt + role_prompt
        
        return base_prompt
    
    # Create uploads directory if it doesn't exist
    os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
    
    from flask import send_from_directory
    
    @app.route('/')
    def index():
        return send_from_directory('.', 'index.html')
    
    @app.route('/api/status')
    def status():
        """Check system status"""
        is_connected, message = check_connection()
        return jsonify({
            'connected': is_connected,
            'message': message,
            'models': get_available_models()
        })
    
    @app.route('/api/analyze', methods=['POST'])
    def analyze():
        """Analyze uploaded image"""
        try:
            if 'image' not in request.files:
                return jsonify({'error': 'No image file provided'}), 400
            
            file = request.files['image']
            prompt = request.form.get('prompt', 'Describe this image in detail')
            
            if file.filename == '':
                return jsonify({'error': 'No image selected'}), 400
            
            if file and file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):
                # Use the image data directly
                result = analyze_image(file, prompt)
                
                return jsonify({
                    'success': True,
                    'analysis': result,
                    'prompt': prompt
                })
            else:
                return jsonify({'error': 'Invalid image format'}), 400
                
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/chat', methods=['POST'])
    def chat_endpoint():
        """Chat with Gemma with session memory management"""
        try:
            data = request.json
            prompt = data.get('prompt', '')
            username = data.get('username', '')
            role = data.get('role', '')
            
            if not prompt:
                return jsonify({'error': 'No prompt provided'}), 400
            
            # Get session ID for conversation tracking
            session_id = get_session_id()
            
            # Check if user wants to clear the conversation
            if detect_clear_command(prompt):
                clear_session_context(session_id)
                return jsonify({
                    'success': True,
                    'response': "Got it! This conversation has been cleared. Let's start fresh.",
                    'session_cleared': True
                })
            
            # Check if user is referencing past sessions
            if detect_memory_references(prompt):
                memory_response = "I'm sorry, I don't have memory of past chats. Could you tell me again?"
                
                # Add this exchange to session context
                add_to_session_context(session_id, prompt, memory_response)
                
                return jsonify({
                    'success': True,
                    'response': memory_response,
                    'memory_reference_detected': True
                })
            
            # Get current session context (only from this session)
            session_context = get_session_context(session_id)
            
            # Build context from current session only
            context_messages = []
            for msg in session_context:
                context_messages.append(f"User: {msg['user']}")
                context_messages.append(f"Assistant: {msg['assistant']}")
            
            session_context_str = "\n".join(context_messages) if context_messages else ""
            
            # Generate system message with memory constraints
            system_message = generate_memory_constraint_prompt(username, role)
            
            # Add session context as additional context
            full_context = f"{session_context_str}\n\nCurrent conversation context (this session only): {session_context_str}" if session_context_str else ""
            
            # Get AI response
            result = chat(prompt, full_context, system_message)
            logger.info(f"AI response: {result}")
            
            # Process AI response for asterisk formatting if formatter is available
            formatted_response = result
            asterisk_detection = None
            
            if AI_FORMATTER_AVAILABLE and ai_formatter:
                try:
                    formatting_result = ai_formatter.process_ai_response(result)
                    formatted_response = formatting_result['formatted_text']
                    asterisk_detection = {
                        'detected': formatting_result['detection_result']['has_asterisks'],
                        'is_ai_emphasis': formatting_result['detection_result']['is_ai_emphasis'],
                        'confidence': formatting_result['detection_result']['confidence_score'],
                        'formatting_types': formatting_result['detection_result']['formatting_types'],
                        'processing_notes': formatting_result['processing_notes']
                    }
                    
                    if formatting_result['detection_result']['has_asterisks']:
                        logger.info(f"‚ú® Asterisk formatting detected in AI response (confidence: {formatting_result['detection_result']['confidence_score']:.2f})")
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Asterisk formatting failed: {e}")
            
            # Add this exchange to session context (use original response for context)
            add_to_session_context(session_id, prompt, result)
            
            response_data = {
                'success': True,
                'response': formatted_response,
                'session_id': session_id
            }
            
            # Add asterisk detection info if available
            if asterisk_detection:
                response_data['asterisk_detection'] = asterisk_detection
            
            return jsonify(response_data)
            
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    
    
    @app.route('/favicon.ico')
    def favicon():
        return '', 204
    
    @app.route('/api/health')
    def health():
        """Health check endpoint"""
        return jsonify({'status': 'healthy'})
    
    @app.route('/api/system-status')
    def system_status_endpoint():
        """Get comprehensive system status"""
        try:
            # Get system initialization status
            system_status = initialize_comprehensive_system()
            
            # Get system info
            system_info = get_system_info()
            
            # Check Ollama connection if available
            ollama_status = {'connected': False, 'message': 'Not available'}
            if GEMMA_AVAILABLE:
                try:
                    ollama_status['connected'], ollama_status['message'] = check_connection()
                    ollama_status['models'] = get_available_models()
                except Exception as e:
                    ollama_status['error'] = str(e)
            
            # Check session status
            session_id = get_session_id()
            session_context = get_session_context(session_id)
            
            return jsonify({
                'system_status': system_status,
                'system_info': system_info,
                'ollama_status': ollama_status,
                'session_info': {
                    'session_id': session_id,
                    'messages_in_context': len(session_context),
                    'total_sessions': len(SESSION_CONVERSATIONS),
                    'memory_policy': 'Session-only memory'
                },
                'endpoints': {
                    'chat': '/api/chat',
                    'voice_chat': '/api/voice-chat',
                    'image_analysis': '/api/analyze',
                    'enhanced_vision': '/api/enhanced-vision',
                    'voice_interaction': '/api/voice-interact',
                    'vosk_transcribe': '/api/vosk-transcribe',
                    'user_management': ['/api/user-fetch', '/api/user-create', '/api/user-update-role']
                },
                'features_available': {
                    'regular_chat': True,
                    'voice_chat': VOICE_PROCESSING_AVAILABLE,
                    'image_analysis': GEMMA_AVAILABLE,
                    'enhanced_vision': GEMMA_INTEGRATION_AVAILABLE,
                    'offline_assistant': VEDIX_AVAILABLE,
                    'user_profiles': True,
                    'session_memory': True
                },
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"System status check failed: {e}")
            return jsonify({
                'error': str(e),
                'basic_status': 'Flask app running',
                'timestamp': datetime.now().isoformat()
            }), 500
    
    # -------------------
    # Enhanced user profile system with persistent storage
    import json
    from datetime import datetime
    
    # Load user profiles from file
    def load_user_profiles():
        try:
            with open('user_profiles.json', 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            return {}
    
    # Save user profiles to file
    def save_user_profiles(profiles):
        with open('user_profiles.json', 'w') as f:
            json.dump(profiles, f, indent=2)
    
    USER_PROFILES = load_user_profiles()
    
    @app.route('/api/user-fetch')
    def user_fetch():
        """Fetch user profile by name or ID"""
        name = request.args.get('name', '').strip()
        user_id = request.args.get('id', '').strip()
        
        if not name and not user_id:
            return jsonify({'exists': False, 'error': 'Name or ID required'}), 400
        
        # Search by name or ID
        key = name.lower() if name else user_id
        user = USER_PROFILES.get(key)
        
        if user:
            return jsonify({
                'exists': True, 
                'name': user['name'],
                'role': user['role'],
                'created_at': user.get('created_at'),
                'updated_at': user.get('updated_at'),
                'interaction_count': user.get('interaction_count', 0)
            })
        else:
            return jsonify({'exists': False})
    
    @app.route('/api/user-create', methods=['POST'])
    def user_create():
        """Create or update user profile"""
        data = request.json
        name = data.get('name', '').strip()
        role = data.get('role', '').strip()
        user_id = data.get('id', '').strip()
        
        if not name or not role:
            return jsonify({'success': False, 'error': 'Missing name or role'}), 400
        
        # Use name as key (lowercase for consistency) or provided ID
        key = user_id if user_id else name.lower()
        
        now = datetime.now().isoformat()
        
        if key in USER_PROFILES:
            # Update existing user
            USER_PROFILES[key]['role'] = role
            USER_PROFILES[key]['updated_at'] = now
            USER_PROFILES[key]['interaction_count'] = USER_PROFILES[key].get('interaction_count', 0) + 1
        else:
            # Create new user
            USER_PROFILES[key] = {
                'name': name,
                'role': role,
                'created_at': now,
                'updated_at': now,
                'interaction_count': 1
            }
        
        save_user_profiles(USER_PROFILES)
        return jsonify({'success': True, 'user_id': key})
    
    @app.route('/api/user-update-role', methods=['POST'])
    def user_update_role():
        """Update user's relationship role"""
        data = request.json
        name = data.get('name', '').strip()
        new_role = data.get('role', '').strip()
        user_id = data.get('id', '').strip()
        
        if not new_role:
            return jsonify({'success': False, 'error': 'Role is required'}), 400
        
        if not name and not user_id:
            return jsonify({'success': False, 'error': 'Name or ID required'}), 400
        
        key = user_id if user_id else name.lower()
        
        if key in USER_PROFILES:
            old_role = USER_PROFILES[key]['role']
            USER_PROFILES[key]['role'] = new_role
            USER_PROFILES[key]['updated_at'] = datetime.now().isoformat()
            save_user_profiles(USER_PROFILES)
            
            return jsonify({
                'success': True, 
                'message': f'Role updated from "{old_role}" to "{new_role}"',
                'old_role': old_role,
                'new_role': new_role
            })
        else:
            return jsonify({'success': False, 'error': 'User not found'}), 404
    
    @app.route('/api/user-stats')
    def user_stats():
        """Get user interaction statistics"""
        name = request.args.get('name', '').strip()
        user_id = request.args.get('id', '').strip()
        
        if not name and not user_id:
            return jsonify({'error': 'Name or ID required'}), 400
        
        key = user_id if user_id else name.lower()
        user = USER_PROFILES.get(key)
        
        if user:
            return jsonify({
                'name': user['name'],
                'role': user['role'],
                'interaction_count': user.get('interaction_count', 0),
                'member_since': user.get('created_at'),
                'last_interaction': user.get('updated_at')
            })
        else:
            return jsonify({'error': 'User not found'}), 404
    
    @app.route('/api/session-status')
    def session_status():
        """Get current session information and memory status"""
        try:
            session_id = get_session_id()
            session_context = get_session_context(session_id)
            
            return jsonify({
                'session_id': session_id,
                'message_count': len(session_context),
                'memory_policy': 'Session-only memory - no cross-session persistence',
                'last_messages': len(session_context),
                'memory_constraints': {
                    'max_context_messages': 10,
                    'max_stored_messages': 20,
                    'cross_session_memory': False,
                    'session_persistence': False
                }
            })
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/voice-chat', methods=['POST'])
    def voice_chat_endpoint():
        """Voice chat with role-based Gemma system (same as regular chat but for voice)"""
        try:
            data = request.json
            voice_text = data.get('voiceText', '')
            username = data.get('username', '')
            role = data.get('role', '')
            voice_enabled = data.get('voice_enabled', False)  # Default: voice OFF
            
            if not voice_text:
                return jsonify({'success': False, 'error': 'No voice text provided'}), 400
            
            # Get session ID for conversation tracking
            session_id = get_session_id()
            
            # Check if user wants to clear the conversation
            if detect_clear_command(voice_text):
                clear_session_context(session_id)
                return jsonify({
                    'success': True,
                    'reply': "Got it! Voice conversation cleared. Let's start fresh.",
                    'recognized_text': voice_text,
                    'session_cleared': True,
                    'voice_personality': 'Gemma'
                })
            
            # Check if user is referencing past sessions
            if detect_memory_references(voice_text):
                memory_response = "I'm sorry, I don't have memory of past voice chats. Could you tell me again?"
                
                # Add this exchange to session context
                add_to_session_context(session_id, voice_text, memory_response)
                
                return jsonify({
                    'success': True,
                    'reply': memory_response,
                    'recognized_text': voice_text,
                    'memory_reference_detected': True,
                    'voice_personality': 'Gemma'
                })
            
            # Get current session context (only from this session)
            session_context = get_session_context(session_id)
            
            # Build context from current session only
            context_messages = []
            for msg in session_context:
                context_messages.append(f"User: {msg['user']}")
                context_messages.append(f"Assistant: {msg['assistant']}")
            
            session_context_str = "\n".join(context_messages) if context_messages else ""
            
            # Generate system message with memory constraints and role-based personality
            system_message = generate_memory_constraint_prompt(username, role)
            
            # Add session context as additional context
            full_context = f"{session_context_str}\n\nCurrent conversation context (this session only): {session_context_str}" if session_context_str else ""
            
            # Get AI response using the same chat system
            result = chat(voice_text, full_context, system_message)
            
            # Add this exchange to session context
            add_to_session_context(session_id, voice_text, result)
            
            return jsonify({
                'success': True,
                'reply': result,
                'recognized_text': voice_text,
                'session_id': session_id,
                'voice_personality': 'Gemma',
                'username': username,
                'role': role,
                'voice_enabled': voice_enabled
            })
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/voice-chat-vedx-lite', methods=['POST'])
    def voice_chat_vedx_lite_endpoint():
        """Voice chat with Vedx Lite personality - supportive companion for introverts"""
        try:
            data = request.json
            voice_text = data.get('voiceText', '')
            username = data.get('username', '')
            voice_enabled = data.get('voice_enabled', True)
            
            if not voice_text:
                return jsonify({'success': False, 'error': 'No voice text provided'}), 400
            
            # Get session ID for conversation tracking
            session_id = get_session_id()
            
            # Check if user wants to clear the conversation
            if detect_clear_command(voice_text):
                clear_session_context(session_id)
                return jsonify({
                    'success': True,
                    'reply': "Got it! This voice conversation has been cleared. Let's start fresh. I'm here to support you. üíô",
                    'recognized_text': voice_text,
                    'session_cleared': True,
                    'vedx_lite': True,
                    'voice_personality': 'Vedx Lite'
                })
            
            # Get current session context
            session_context = get_session_context(session_id)
            
            # Build context from current session only
            context_messages = []
            for msg in session_context:
                context_messages.append(f"User: {msg['user']}")
                context_messages.append(f"Vedx Lite: {msg['assistant']}")
            
            session_context_str = "\n".join(context_messages) if context_messages else ""
            
            # Get Vedx Lite response with voice control
            result = chat_vedx_lite(voice_text, session_context_str, voice_enabled)
            
            # Add this exchange to session context
            add_to_session_context(session_id, voice_text, result)
            
            return jsonify({
                'success': True,
                'reply': result,
                'recognized_text': voice_text,
                'session_id': session_id,
                'vedx_lite': True,
                'voice_personality': 'Vedx Lite',
                'username': username,
                'voice_enabled': voice_enabled
            })
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/chat-image', methods=['POST'])
    def chat_image_endpoint():
        """Chat with image using Gemma"""
        try:
            if 'image' not in request.files:
                return jsonify({'error': 'No image file provided'}), 400
            
            file = request.files['image']
            prompt = request.form.get('prompt', 'Describe this image')
            username = request.form.get('username', '')
            role = request.form.get('role', '')
            
            if file.filename == '':
                return jsonify({'error': 'No image selected'}), 400
            
            if file and file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):
                # Create personalized prompt based on user's relationship preference
                if username and role:
                    personalized_prompt = f"""You are speaking with {username}. They see you as their {role}. Adapt your response accordingly:
    - Best Friend: Be casual, supportive, use friendly language and emojis
    - Motivator: Be encouraging, energetic, focus on achievements
    - Female Friend: Be warm, caring, understanding, and empathetic
    - Friend: Be helpful, kind, and approachable
    - Guide: Be knowledgeable, patient, and instructional
    
    User's message: {prompt}"""
                else:
                    personalized_prompt = prompt
                
                result = analyze_image(file, personalized_prompt)
                
                return jsonify({
                    'success': True,
                    'response': result
                })
            else:
                return jsonify({'error': 'Invalid image format'}), 400
                
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    # Initialize VediX
    vedix = get_vedix()
    
    @app.route('/api/voice-interact', methods=['POST'])
    def voice_interact():
        """VediX voice interaction endpoint - handles both audio files and direct text"""
        try:
            # Handle JSON text input (from our voice recognition)
            if request.is_json:
                data = request.get_json()
                voice_text = data.get('voiceText', '')
                username = data.get('username', 'Utkarsh')
                role = data.get('role', 'Friend')
                
                if not voice_text:
                    return jsonify({'success': False, 'error': 'No voice text provided'}), 400
                
                # Process with VediX
                vedix_response = vedix.process_voice_command(voice_text)
                
                # Personalize the response with the username
                if username and username.lower() != 'utkarsh':
                    # Add personal touch if name isn't already included
                    if any(greeting in vedix_response for greeting in ["Hello", "Hi", "Hey"]):
                        vedix_response = vedix_response.replace("Utkarsh", username)
                        vedix_response = vedix_response.replace("Hello", f"Hello, {username}")
                        vedix_response = vedix_response.replace("Hi", f"Hi, {username}")
                        vedix_response = vedix_response.replace("Hey", f"Hey, {username}")
                
                return jsonify({
                    'success': True,
                    'reply': vedix_response,
                    'recognized_text': voice_text,
                    'vedix_active': True,
                    'username': username,
                    'role': role
                })
            
            # Handle audio file input (legacy support)
            elif 'audio' in request.files:
                audio_file = request.files['audio']
                username = request.form.get('username', 'Utkarsh')
                role = request.form.get('role', 'Friend')
                
                # Save uploaded audio to a temp file
                fd, temp_audio_path = tempfile.mkstemp(suffix='.webm')
                try:
                    audio_file.save(temp_audio_path)
                    
                    # For demo purposes, simulate voice recognition
                    import random
                    demo_commands = [
                        "hello", "what time is it", "tell me a joke", 
                        "how are you", "what can you do", "thank you"
                    ]
                    
                    recognized_text = random.choice(demo_commands)
                    vedix_response = vedix.process_voice_command(recognized_text)
                    
                    # Personalize the response
                    if username and username.lower() != 'utkarsh':
                        vedix_response = vedix_response.replace("Utkarsh", username)
                    
                    return jsonify({
                        'success': True,
                        'reply': vedix_response,
                        'recognized_text': recognized_text,
                        'vedix_active': True,
                        'username': username,
                        'role': role
                    })
                    
                finally:
                    os.close(fd)
                    os.remove(temp_audio_path)
            
            else:
                return jsonify({'success': False, 'error': 'No audio file or voice text provided'}), 400
                
        except Exception as e:
            # Fallback VediX response
            username = 'Utkarsh'
            try:
                if request.is_json:
                    data = request.get_json()
                    username = data.get('username', 'Utkarsh')
                elif request.form:
                    username = request.form.get('username', 'Utkarsh')
            except:
                pass
                
            return jsonify({
                'success': True,
                'reply': f"Hello, {username}! I'm VediX, your offline AI assistant. I work fully offline to help you anytime. How can I assist you today?",
                'error': f"Processing note: {str(e)}",
                'vedix_active': True,
                'username': username
            })
    
    @app.route('/api/vedix-greeting')
    def vedix_greeting():
        """Get VediX initial greeting"""
        try:
            username = request.args.get('username', 'Utkarsh')
            greeting = vedix.get_greeting()
            
            # Personalize greeting
            if username and username != 'Utkarsh':
                greeting = greeting.replace('Utkarsh', username)
            
            return jsonify(
                success=True,
                greeting=greeting,
                vedix_active=True
            )
        except Exception as e:
            return jsonify(
                success=True,
                greeting="Hello, Utkarsh! I'm VediX, your offline AI assistant. How can I help you today?",
                error=str(e)
            )
    
    # ===== ENHANCED GEMMA3N INTEGRATION ENDPOINTS =====
    # These endpoints use the new reasoning layer when available
    
    @app.route('/api/enhanced-vision', methods=['POST'])
    def enhanced_vision_endpoint():
        """Enhanced vision processing with Gemma3n reasoning layer"""
        try:
            if 'image' not in request.files:
                return jsonify({'error': 'No image file provided'}), 400
            
            file = request.files['image']
            user_question = request.form.get('prompt', '')
            username = request.form.get('username', '')
            role = request.form.get('role', '')
            
            if file.filename == '':
                return jsonify({'error': 'No image selected'}), 400
            
            if file and file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')):
                # First get basic vision description using existing system
                vision_description = analyze_image(file, "Describe this image clearly and systematically for a blind person, focusing on important objects, their locations, and any potential hazards or useful information.")
                
                # Use enhanced reasoning if available
                if GEMMA_INTEGRATION_AVAILABLE and is_gemma_enabled():
                    try:
                        reasoning_layer = get_reasoning_layer()
                        
                        # Prepare additional context
                        additional_context = {
                            'username': username,
                            'role': role,
                            'interaction_type': 'vision_processing'
                        }
                        
                        # Process with reasoning layer
                        result = reasoning_layer.process_vision_input(
                            vision_description=vision_description,
                            user_question=user_question,
                            template_type='vision_description',
                            additional_context=additional_context
                        )
                        
                        return jsonify({
                            'success': result['success'],
                            'response': result['response'],
                            'vision_description': vision_description,
                            'enhanced_processing': True,
                            'metadata': result.get('metadata', {})
                        })
                        
                    except Exception as e:
                        logging.error(f"Enhanced vision processing failed: {e}")
                        # Fall back to basic response
                        pass
                
                # Fallback to basic processing
                if user_question:
                    response = f"I can see: {vision_description}\n\nRegarding your question '{user_question}': Based on what I observe, I recommend being careful and taking your time to safely navigate or interact with the objects in the scene."
                else:
                    response = f"I can see: {vision_description}"
                
                return jsonify({
                    'success': True,
                    'response': response,
                    'vision_description': vision_description,
                    'enhanced_processing': False,
                    'note': 'Using basic vision processing - enhanced reasoning unavailable'
                })
            else:
                return jsonify({'error': 'Invalid image format'}), 400
                
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/enhanced-voice', methods=['POST'])
    def enhanced_voice_endpoint():
        """Enhanced voice processing with Gemma3n reasoning layer"""
        try:
            data = request.json
            voice_text = data.get('voiceText', '')
            scene_context = data.get('sceneContext', '')
            username = data.get('username', '')
            role = data.get('role', '')
            
            if not voice_text:
                return jsonify({'success': False, 'error': 'No voice text provided'}), 400
            
            # Use enhanced reasoning if available
            if GEMMA_INTEGRATION_AVAILABLE and is_gemma_enabled():
                try:
                    reasoning_layer = get_reasoning_layer()
                    
                    # Determine template type based on voice content
                    voice_lower = voice_text.lower()
                    if any(word in voice_lower for word in ['navigate', 'direction', 'where', 'go', 'move']):
                        template_type = 'navigation_help'
                    elif any(word in voice_lower for word in ['see', 'describe', 'what', 'identify']):
                        template_type = 'vision_description'
                    else:
                        template_type = 'general_assistance'
                    
                    # Process with reasoning layer
                    result = reasoning_layer.process_voice_input(
                        voice_text=voice_text,
                        scene_context=scene_context,
                        template_type=template_type
                    )
                    
                    return jsonify({
                        'success': result['success'],
                        'reply': result['response'],
                        'recognized_text': voice_text,
                        'enhanced_processing': True,
                        'template_type': template_type,
                        'metadata': result.get('metadata', {})
                    })
                    
                except Exception as e:
                    logging.error(f"Enhanced voice processing failed: {e}")
                    # Fall back to basic response
                    pass
            
            # Fallback to basic voice processing using VediX
            vedix_response = vedix.process_voice_command(voice_text)
            
            return jsonify({
                'success': True,
                'reply': vedix_response,
                'recognized_text': voice_text,
                'enhanced_processing': False,
                'note': 'Using basic voice processing - enhanced reasoning unavailable'
            })
            
        except Exception as e:
            return jsonify({'success': False, 'error': str(e)}), 500
    
    @app.route('/api/gemma-status')
    def gemma_status_endpoint():
        """Get Gemma3n reasoning layer status"""
        try:
            if not GEMMA_INTEGRATION_AVAILABLE:
                return jsonify({
                    'available': False,
                    'message': 'Enhanced Gemma3n integration not loaded'
                })
            
            config = get_config()
            reasoning_layer = get_reasoning_layer()
            status = reasoning_layer.get_system_status()
            
            return jsonify({
                'available': True,
                'enabled': is_gemma_enabled(),
                'model_name': config.GEMMA_MODEL_NAME,
                'system_status': status,
                'config': config.get_gemma_config()
            })
            
        except Exception as e:
            return jsonify({
                'available': False,
                'error': str(e)
            }), 500
    
    @app.route('/api/gemma-toggle', methods=['POST'])
    def gemma_toggle_endpoint():
        """Toggle Gemma3n reasoning layer on/off"""
        try:
            if not GEMMA_INTEGRATION_AVAILABLE:
                return jsonify({
                    'success': False,
                    'error': 'Enhanced Gemma3n integration not available'
                }), 400
            
            data = request.json
            enable = data.get('enable', True)
            
            config = get_config()
            reasoning_layer = get_reasoning_layer()
            
            # Toggle both config and reasoning layer
            config.toggle_gemma(enable)
            reasoning_layer.toggle_gemma(enable)
            
            return jsonify({
                'success': True,
                'enabled': enable,
                'message': f'Gemma3n reasoning layer {"enabled" if enable else "disabled"}'
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    @app.route('/api/test-asterisk-detection', methods=['POST'])
    def test_asterisk_detection_endpoint():
        """Test endpoint for asterisk detection system"""
        try:
            data = request.json
            test_text = data.get('text', '')
            
            if not test_text:
                return jsonify({'error': 'No text provided for testing'}), 400
            
            if not AI_FORMATTER_AVAILABLE or not ai_formatter:
                return jsonify({
                    'error': 'AI Response Formatter not available',
                    'system_available': False
                }), 503
            
            # Process the text with asterisk detection
            result = ai_formatter.process_ai_response(test_text, format_output=True)
            
            return jsonify({
                'success': True,
                'input_text': test_text,
                'detection_result': result['detection_result'],
                'formatted_text': result['formatted_text'],
                'processing_notes': result['processing_notes'],
                'system_available': True,
                'formatting_applied': result['formatted_text'] != result['original_text']
            })
            
        except Exception as e:
            return jsonify({
                'success': False,
                'error': str(e)
            }), 500
    
    @app.route('/api/vosk-transcribe', methods=['POST'])
    def vosk_transcribe():
        """Vosk speech-to-text transcription endpoint"""
        try:
            if 'audio' not in request.files:
                return jsonify({'success': False, 'error': 'No audio file provided'}), 400
            
            audio_file = request.files['audio']
            
            if audio_file.filename == '':
                return jsonify({'success': False, 'error': 'No audio file selected'}), 400
            
            # Save the audio file temporarily
            import tempfile
            import wave
            import json
            
            # Create a temporary file for the audio
            fd, temp_audio_path = tempfile.mkstemp(suffix='.wav')
            
            try:
                # Save the uploaded audio
                audio_file.save(temp_audio_path)
                
                # Initialize Vosk if not already done
                try:
                    import vosk
                    import os
                    
                    # Check if model exists
                    model_path = "model/vosk-model-small-en-us-0.15"
                    if not os.path.exists(model_path):
                        return jsonify({
                            'success': False, 
                            'error': f'Vosk model not found at {model_path}'
                        }), 500
                    
                    # Load Vosk model
                    model = vosk.Model(model_path)
                    rec = vosk.KaldiRecognizer(model, 16000)
                    rec.SetWords(True)
                    
                    # Process audio file
                    with wave.open(temp_audio_path, 'rb') as wf:
                        # Check audio format
                        if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != "NONE":
                            return jsonify({
                                'success': False, 
                                'error': 'Audio must be WAV format mono PCM.'
                            }), 400
                        
                        # Read and process audio data
                        results = []
                        while True:
                            data = wf.readframes(4000)
                            if len(data) == 0:
                                break
                            if rec.AcceptWaveform(data):
                                result = json.loads(rec.Result())
                                if 'text' in result and result['text'].strip():
                                    results.append(result['text'].strip())
                        
                        # Get final result
                        final_result = json.loads(rec.FinalResult())
                        if 'text' in final_result and final_result['text'].strip():
                            results.append(final_result['text'].strip())
                        
                        # Combine all results
                        transcription = ' '.join(results).strip()
                        
                        return jsonify({
                            'success': True,
                            'text': transcription,
                            'confidence': 0.9  # Mock confidence score
                        })
                        
                except ImportError:
                    # Fallback: Use a mock transcription service for demo
                    import random
                    demo_responses = [
                        "hello", "how are you", "what time is it", "thank you",
                        "tell me a joke", "what can you do", "help me", "good morning"
                    ]
                    
                    mock_text = random.choice(demo_responses)
                    return jsonify({
                        'success': True,
                        'text': mock_text,
                        'confidence': 0.8,
                        'note': 'Using mock transcription - install vosk-api for real transcription'
                    })
                    
            finally:
                # Clean up temporary file
                os.close(fd)
                if os.path.exists(temp_audio_path):
                    os.remove(temp_audio_path)
                    
        except Exception as e:
            return jsonify({
                'success': False, 
                'error': f'Transcription failed: {str(e)}',
                'fallback_text': 'hello'  # Fallback text
            }), 500
    
    if __name__ == '__main__':
        print("üöÄ Starting Comprehensive AI Assistant...")
        print("üìç Open http://localhost:5000 in your browser")
        
        # Initialize comprehensive system
        print("\nüîß Initializing Comprehensive AI System...")
        system_status = initialize_comprehensive_system()
        
        # Display system status
        print("\nüìä SYSTEM STATUS REPORT")
        print("=" * 50)
        
        # Core components
        print("üîå Core Components:")
        print(f"   ‚Ä¢ Gemma AI: {'‚úÖ Available' if system_status['gemma_ai'] else '‚ùå Unavailable'}")
        print(f"   ‚Ä¢ VediX Offline: {'‚úÖ Available' if system_status['vedix_offline'] else '‚ùå Unavailable'}")
        print(f"   ‚Ä¢ Enhanced Reasoning: {'‚úÖ Available' if system_status['enhanced_reasoning'] else '‚ùå Unavailable'}")
        print(f"   ‚Ä¢ Voice Processing: {'‚úÖ Available' if system_status['voice_processing'] else '‚ùå Unavailable'}")
        print(f"   ‚Ä¢ Profile Management: {'‚úÖ Available' if system_status['profile_management'] else '‚ùå Unavailable'}")
        
        # External connections
        print("\nüåê External Connections:")
        print(f"   ‚Ä¢ Ollama Connection: {'‚úÖ Connected' if system_status['ollama_connection'] else '‚ùå Not Connected'}")
        print(f"   ‚Ä¢ Vosk Model: {'‚úÖ Found' if system_status['vosk_model'] else '‚ùå Not Found'}")
        
        # Features available
        print("\nüéÜ Available Features:")
        features = [
            ("üí¨ Regular Chat", True),
            ("üñºÔ∏è Image Analysis", system_status['gemma_ai']),
            ("üé§ Voice Interaction", system_status['voice_processing']),
            ("üîÆ Enhanced Vision", system_status['enhanced_vision']),
            ("ü§ñ Offline Assistant", system_status['vedix_offline']),
            ("üë§ User Profiles", True),
            ("üß† Session Memory", True)
        ]
        
        for feature_name, is_available in features:
            status_icon = "‚úÖ" if is_available else "‚ö†Ô∏è"
            print(f"   ‚Ä¢ {feature_name}: {status_icon}")
        
        # API endpoints
        print("\nüîó Available API Endpoints:")
        endpoints = [
            "/api/chat - Text chat with AI",
            "/api/voice-chat - Voice chat with AI", 
            "/api/analyze - Image analysis",
            "/api/enhanced-vision - Enhanced vision processing",
            "/api/voice-interact - VediX voice interaction",
            "/api/vosk-transcribe - Speech-to-text",
            "/api/system-status - System status",
            "/api/user-fetch - User profile management"
        ]
        
        for endpoint in endpoints:
            print(f"   ‚Ä¢ {endpoint}")
        
        # Check Ollama connection
        if GEMMA_AVAILABLE:
            try:
                is_connected, message = check_connection()
                if is_connected:
                    print(f"\nüåê Ollama Status: ‚úÖ {message}")
                    models = get_available_models()
                    if models:
                        print(f"   Available models: {', '.join(models)}")
                else:
                    print(f"\nüåê Ollama Status: ‚ùå {message}")
                    print("   ‚ö†Ô∏è  Make sure Ollama is running and gemma3n:latest is installed")
            except Exception as e:
                print(f"\nüåê Ollama Status: ‚ùå Connection check failed: {e}")
        
        print("\n" + "=" * 50)
        print("üåü Comprehensive AI Assistant is ready!")
        print(f"   üìã Logs: app.log")
        print(f"   üìÅ Data: user_profiles.json")
        print(f"   üîç System: /api/system-status")
        
        # Final status summary
        active_features = sum(1 for _, available in features if available)
        total_features = len(features)
        print(f"   üìä Features Active: {active_features}/{total_features}")
        
        app.run(debug=True, host='0.0.0.0', port=5000)
----- [END OF main.py] -----

üîπ 16. optimize_ollama.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\optimize_ollama.py
----- [START OF optimize_ollama.py] -----
    #!/usr/bin/env python3
    """
    Ollama Optimization and Performance Checker
    ===========================================
    This script helps optimize Ollama performance and provides model recommendations.
    """
    
    import requests
    import json
    import time
    import subprocess
    import sys
    from typing import List, Dict, Any, Tuple
    
    class OllamaOptimizer:
        def __init__(self, ollama_url: str = "http://localhost:11434"):
            self.ollama_url = ollama_url
            self.api_endpoint = f"{ollama_url}/api"
            
        def check_ollama_status(self) -> Tuple[bool, str]:
            """Check if Ollama is running and responsive."""
            try:
                response = requests.get(f"{self.api_endpoint}/tags", timeout=10)
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    return True, f"Ollama running with {len(models)} models"
                else:
                    return False, f"Ollama responded with status {response.status_code}"
            except requests.RequestException as e:
                return False, f"Ollama not accessible: {str(e)}"
        
        def list_available_models(self) -> List[Dict[str, Any]]:
            """List all available models with their sizes."""
            try:
                response = requests.get(f"{self.api_endpoint}/tags", timeout=10)
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    model_info = []
                    for model in models:
                        size_gb = model.get('size', 0) / (1024**3)  # Convert to GB
                        model_info.append({
                            'name': model.get('name', 'Unknown'),
                            'size_gb': round(size_gb, 2),
                            'modified': model.get('modified_at', 'Unknown')
                        })
                    return sorted(model_info, key=lambda x: x['size_gb'])
                return []
            except Exception as e:
                print(f"Error listing models: {e}")
                return []
        
        def test_model_performance(self, model_name: str, prompt: str = "Hello, how are you?") -> Dict[str, Any]:
            """Test model performance with a simple prompt."""
            print(f"Testing {model_name} performance...")
            
            payload = {
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 100  # Short response for testing
                }
            }
            
            start_time = time.time()
            try:
                response = requests.post(
                    f"{self.api_endpoint}/generate",
                    json=payload,
                    timeout=120  # 2 minute timeout for testing
                )
                end_time = time.time()
                
                if response.status_code == 200:
                    result = response.json()
                    response_text = result.get('response', '')
                    return {
                        'success': True,
                        'response_time': round(end_time - start_time, 2),
                        'response_length': len(response_text),
                        'response_preview': response_text[:100] + '...' if len(response_text) > 100 else response_text
                    }
                else:
                    return {
                        'success': False,
                        'error': f"HTTP {response.status_code}: {response.text}",
                        'response_time': round(end_time - start_time, 2)
                    }
            except requests.Timeout:
                return {
                    'success': False,
                    'error': "Request timed out (2 minutes)",
                    'response_time': 120
                }
            except Exception as e:
                return {
                    'success': False,
                    'error': str(e),
                    'response_time': round(time.time() - start_time, 2)
                }
        
        def recommend_best_model(self) -> Dict[str, Any]:
            """Test all models and recommend the best one for vision processing."""
            models = self.list_available_models()
            if not models:
                return {'error': 'No models available'}
            
            print(f"\nTesting {len(models)} models for performance...")
            test_prompt = "You are a helpful AI assistant. Describe what you see in this scene: A table with objects on it."
            
            results = []
            for model in models:
                model_name = model['name']
                performance = self.test_model_performance(model_name, test_prompt)
                
                # Calculate score based on success, speed, and size
                if performance['success']:
                    # Prefer faster models, but not too small
                    speed_score = max(0, 60 - performance['response_time'])  # Prefer < 60s
                    size_score = min(20, model['size_gb'])  # Prefer reasonable size
                    total_score = speed_score + (size_score / 2)
                else:
                    total_score = 0
                
                results.append({
                    'model': model_name,
                    'size_gb': model['size_gb'],
                    'performance': performance,
                    'score': total_score
                })
            
            # Sort by score (highest first)
            results.sort(key=lambda x: x['score'], reverse=True)
            
            return {
                'recommended': results[0] if results else None,
                'all_results': results
            }
        
        def install_lightweight_model(self) -> bool:
            """Install a lightweight model for better performance."""
            lightweight_models = [
                "gemma:2b",      # 2 billion parameters
                "llama3.2:1b",   # 1 billion parameters  
                "llama3.2:3b",   # 3 billion parameters
            ]
            
            print("Installing lightweight model for better performance...")
            
            for model in lightweight_models:
                try:
                    print(f"Trying to install {model}...")
                    result = subprocess.run(
                        ["ollama", "pull", model],
                        capture_output=True,
                        text=True,
                        timeout=300  # 5 minute timeout
                    )
                    
                    if result.returncode == 0:
                        print(f"‚úÖ Successfully installed {model}")
                        return True
                    else:
                        print(f"‚ùå Failed to install {model}: {result.stderr}")
                except subprocess.TimeoutExpired:
                    print(f"‚è∞ Timeout installing {model}")
                except Exception as e:
                    print(f"‚ùå Error installing {model}: {e}")
            
            print("‚ùå Failed to install any lightweight models")
            return False
    
    def main():
        print("üîß Ollama Optimization and Performance Checker")
        print("=" * 50)
        
        optimizer = OllamaOptimizer()
        
        # Check Ollama status
        is_running, status_msg = optimizer.check_ollama_status()
        print(f"üì° Ollama Status: {status_msg}")
        
        if not is_running:
            print("‚ùå Ollama is not running. Please start Ollama and try again.")
            print("   Run: ollama serve")
            return
        
        # List available models
        models = optimizer.list_available_models()
        if not models:
            print("‚ùå No models found. Installing a lightweight model...")
            if optimizer.install_lightweight_model():
                models = optimizer.list_available_models()
        
        if models:
            print(f"\nüìã Available Models ({len(models)}):")
            for model in models:
                print(f"   ‚Ä¢ {model['name']} ({model['size_gb']} GB)")
        
        # Test performance and get recommendations
        if len(models) > 0:
            print("\nüß™ Running Performance Tests...")
            recommendation = optimizer.recommend_best_model()
            
            if 'error' in recommendation:
                print(f"‚ùå Error: {recommendation['error']}")
                return
            
            best_model = recommendation['recommended']
            if best_model and best_model['performance']['success']:
                print(f"\nüèÜ Recommended Model: {best_model['model']}")
                print(f"   ‚Ä¢ Size: {best_model['size_gb']} GB")
                print(f"   ‚Ä¢ Response Time: {best_model['performance']['response_time']}s")
                print(f"   ‚Ä¢ Score: {best_model['score']:.1f}")
                
                # Show configuration recommendation
                print(f"\n‚öôÔ∏è  Configuration Recommendation:")
                print(f"   Set GEMMA_MODEL_NAME='{best_model['model']}'")
                if best_model['performance']['response_time'] > 30:
                    print(f"   Set GEMMA_TIMEOUT=120  # Increase timeout")
                
                # Create a simple config update
                config_update = f"""
    # Add to your environment or config:
    export GEMMA_MODEL_NAME='{best_model['model']}'
    export GEMMA_TIMEOUT={max(60, int(best_model['performance']['response_time']) + 30)}
    export GEMMA_MAX_RETRIES=2
    """
                
                with open('ollama_config_recommendation.txt', 'w') as f:
                    f.write(config_update)
                print(f"   üìÑ Config saved to: ollama_config_recommendation.txt")
            
            else:
                print("\n‚ùå No models performed successfully")
                print("   Consider installing a lighter model:")
                print("   ‚Ä¢ ollama pull gemma:2b")
                print("   ‚Ä¢ ollama pull llama3.2:1b")
        
        # Performance summary
        print(f"\nüìä Performance Summary:")
        all_results = recommendation.get('all_results', [])
        for result in all_results[:3]:  # Show top 3
            status = "‚úÖ" if result['performance']['success'] else "‚ùå"
            print(f"   {status} {result['model']}: {result['performance']['response_time']}s")
        
        print(f"\n‚ú® Optimization complete!")
        print(f"   ‚Ä¢ Restart your application to use the optimized settings")
        print(f"   ‚Ä¢ Monitor performance through the /api/system-status endpoint")
    
    if __name__ == "__main__":
        main()
----- [END OF optimize_ollama.py] -----

üîπ 17. prompt_builder.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\prompt_builder.py
----- [START OF prompt_builder.py] -----
    """
    Gemma3n Prompt Builder
    ======================
    Constructs intelligent prompts for the Gemma3n reasoning layer.
    """
    
    import logging
    from datetime import datetime
    from typing import Optional, Dict, Any
    
    logger = logging.getLogger(__name__)
    
    class GemmaPromptBuilder:
        """
        Builds structured prompts for Gemma3n with context awareness.
        """
        
        def __init__(self):
            self.default_system_prompt = "You are a helpful AI assistant for a blind person."
            
            # Pre-defined prompt templates for different scenarios
            self.templates = {
                'vision_description': {
                    'system': "You are a helpful AI assistant for a blind person. Focus on providing clear, detailed, and actionable descriptions.",
                    'context_prefix': "Scene Description: ",
                    'question_prefix': "User Question: "
                },
                'navigation_help': {
                    'system': "You are a navigation assistant for a blind person. Provide clear, step-by-step guidance focusing on safety and accessibility.",
                    'context_prefix': "Current Scene: ",
                    'question_prefix': "Navigation Request: "
                },
                'object_identification': {
                    'system': "You are an object identification assistant for a blind person. Describe objects clearly, including their location, size, and potential hazards or benefits.",
                    'context_prefix': "Objects in Scene: ",
                    'question_prefix': "User's Question: "
                },
                'general_assistance': {
                    'system': "You are a helpful AI assistant for a blind person. Provide supportive, clear, and practical guidance.",
                    'context_prefix': "Context: ",
                    'question_prefix': "Question: "
                }
            }
        
        def build_prompt(self, 
                        scene_description: str = "", 
                        user_question: str = "", 
                        template_type: str = "general_assistance",
                        custom_system_prompt: Optional[str] = None,
                        additional_context: Optional[Dict[str, Any]] = None) -> str:
            """
            Build a structured prompt for Gemma3n.
            
            Args:
                scene_description: Description of the current scene/image
                user_question: Optional user question or request
                template_type: Type of prompt template to use
                custom_system_prompt: Custom system prompt to override default
                additional_context: Additional context information
                
            Returns:
                Structured prompt string ready for Gemma3n
            """
            try:
                # Get template or use default
                template = self.templates.get(template_type, self.templates['general_assistance'])
                
                # Build system prompt
                system_prompt = custom_system_prompt or template['system']
                
                # Start building the prompt
                prompt_parts = [system_prompt]
                
                # Add timestamp for context
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                prompt_parts.append(f"\nCurrent Time: {timestamp}")
                
                # Add scene description if provided
                if scene_description.strip():
                    context_prefix = template.get('context_prefix', 'Context: ')
                    prompt_parts.append(f"\n{context_prefix}{scene_description.strip()}")
                
                # Add additional context if provided
                if additional_context:
                    for key, value in additional_context.items():
                        if value:
                            prompt_parts.append(f"\n{key}: {value}")
                
                # Add user question if provided
                if user_question.strip():
                    question_prefix = template.get('question_prefix', 'Question: ')
                    prompt_parts.append(f"\n{question_prefix}{user_question.strip()}")
                
                # Add specific instructions based on template type
                if template_type == 'vision_description':
                    prompt_parts.append(self._get_vision_instructions())
                elif template_type == 'navigation_help':
                    prompt_parts.append(self._get_navigation_instructions())
                elif template_type == 'object_identification':
                    prompt_parts.append(self._get_object_identification_instructions())
                
                # Join all parts
                full_prompt = "\n".join(prompt_parts)
                
                logger.info(f"Built prompt for template: {template_type}")
                return full_prompt
                
            except Exception as e:
                logger.error(f"Error building prompt: {e}")
                # Fallback to simple prompt
                return self._build_fallback_prompt(scene_description, user_question)
        
        def _get_vision_instructions(self) -> str:
            """Get specific instructions for vision description tasks."""
            return """
    Instructions:
    - Describe the scene clearly and systematically
    - Start with the overall environment/setting
    - Mention objects from left to right or top to bottom
    - Include important details about colors, sizes, and positions
    - Highlight any potential hazards or obstacles
    - Mention anything that might be useful or important for a blind person
    - Be specific about distances and locations when possible
    - Use clear, descriptive language without complex jargon"""
        
        def _get_navigation_instructions(self) -> str:
            """Get specific instructions for navigation assistance."""
            return """
    Instructions:
    - Prioritize safety above all else
    - Give step-by-step directions
    - Mention obstacles, hazards, or changes in terrain
    - Describe landmarks or reference points
    - Be specific about distances and directions (left, right, forward, back)
    - Suggest the safest path available
    - Warn about stairs, curbs, or elevation changes
    - Mention handrails, walls, or other guidance aids if available"""
        
        def _get_object_identification_instructions(self) -> str:
            """Get specific instructions for object identification."""
            return """
    Instructions:
    - Identify each object clearly and precisely
    - Describe the location of each object relative to the person
    - Mention the approximate size and shape
    - Include color information when relevant
    - Specify if objects are fragile, hot, sharp, or otherwise require caution
    - Mention the purpose or function of objects when helpful
    - Group similar objects together in your description
    - Highlight anything that might be immediately useful or important"""
        
        def _build_fallback_prompt(self, scene_description: str, user_question: str) -> str:
            """Build a simple fallback prompt if the main builder fails."""
            parts = [self.default_system_prompt]
            
            if scene_description.strip():
                parts.append(f"Scene: {scene_description.strip()}")
            
            if user_question.strip():
                parts.append(f"Question: {user_question.strip()}")
            
            return "\n".join(parts)
        
        def get_template_types(self) -> list:
            """Get list of available template types."""
            return list(self.templates.keys())
        
        def add_custom_template(self, name: str, template: Dict[str, str]):
            """Add a custom template."""
            if 'system' not in template:
                template['system'] = self.default_system_prompt
            if 'context_prefix' not in template:
                template['context_prefix'] = 'Context: '
            if 'question_prefix' not in template:
                template['question_prefix'] = 'Question: '
                
            self.templates[name] = template
            logger.info(f"Added custom template: {name}")
    
    # Example usage and testing
    if __name__ == "__main__":
        # Test the prompt builder
        builder = GemmaPromptBuilder()
        
        # Test vision description
        scene = "A table with a bottle and a glass. Chair on the right."
        question = "What should I do?"
        
        prompt = builder.build_prompt(
            scene_description=scene,
            user_question=question,
            template_type="vision_description"
        )
        
        print("Generated Prompt:")
        print("-" * 50)
        print(prompt)
        print("-" * 50)
----- [END OF prompt_builder.py] -----

üîπ 18. reasoning_layer.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\ai_modules\gemma_integration\reasoning_layer.py
----- [START OF reasoning_layer.py] -----
    """
    Gemma Reasoning Layer
    ====================
    High-level reasoning layer that combines prompt building and engine execution.
    """
    
    import logging
    from typing import Optional, Dict, Any, List
    from datetime import datetime
    import json
    
    from .prompt_builder import GemmaPromptBuilder
    from .gemma3n_engine import Gemma3nEngine, get_gemma_engine
    
    logger = logging.getLogger(__name__)
    
    class GemmaReasoningLayer:
        """
        High-level reasoning layer that orchestrates the Vision Assistant's AI responses.
        Combines vision detection results with intelligent prompt building and safe execution.
        """
        
        def __init__(self, 
                     model_name: str = "gemma:2b",  # Use faster model by default
                     enable_gemma: bool = True,
                     log_interactions: bool = True):
            """
            Initialize the reasoning layer.
            
            Args:
                model_name: Name of the Gemma model to use
                enable_gemma: Whether to enable Gemma3n reasoning (True) or use fallbacks (False)
                log_interactions: Whether to log prompts and responses for debugging
            """
            self.enable_gemma = enable_gemma
            self.log_interactions = log_interactions
            self.model_name = model_name
            
            # Initialize components
            self.prompt_builder = GemmaPromptBuilder()
            
            if self.enable_gemma:
                self.engine = get_gemma_engine(model_name)
            else:
                self.engine = None
                
            # Interaction history for debugging
            self.interaction_history = []
            self.max_history_length = 50
            
            logger.info(f"GemmaReasoningLayer initialized - Gemma enabled: {enable_gemma}")
        
        def process_vision_input(self, 
                               vision_description: str,
                               user_question: str = "",
                               template_type: str = "vision_description",
                               additional_context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
            """
            Process vision input and generate intelligent response.
            
            Args:
                vision_description: Description from vision detection (YOLO/BLIP/OCR)
                user_question: Optional user question or request
                template_type: Type of reasoning template to use
                additional_context: Additional context information
                
            Returns:
                Dictionary containing response and metadata
            """
            try:
                start_time = datetime.now()
                
                # Create interaction record
                interaction = {
                    'timestamp': start_time.isoformat(),
                    'input_type': 'vision',
                    'vision_description': vision_description,
                    'user_question': user_question,
                    'template_type': template_type,
                    'additional_context': additional_context,
                    'gemma_enabled': self.enable_gemma
                }
                
                # Generate response
                if self.enable_gemma and self.engine:
                    response = self._generate_gemma_response(
                        vision_description, user_question, template_type, additional_context
                    )
                    interaction['response_source'] = 'gemma3n'
                else:
                    response = self._generate_fallback_response(
                        vision_description, user_question, template_type
                    )
                    interaction['response_source'] = 'fallback'
                
                # Finalize interaction record
                interaction['response'] = response
                interaction['processing_time'] = (datetime.now() - start_time).total_seconds()
                
                # Log interaction if enabled
                if self.log_interactions:
                    self._log_interaction(interaction)
                
                # Return structured response
                return {
                    'success': True,
                    'response': response,
                    'metadata': {
                        'source': interaction['response_source'],
                        'template_type': template_type,
                        'processing_time': interaction['processing_time'],
                        'gemma_enabled': self.enable_gemma
                    }
                }
                
            except Exception as e:
                logger.error(f"Error in process_vision_input: {e}")
                return {
                    'success': False,
                    'response': self._get_error_response(str(e)),
                    'metadata': {
                        'source': 'error_handler',
                        'error': str(e)
                    }
                }
        
        def process_voice_input(self,
                              voice_text: str,
                              scene_context: str = "",
                              template_type: str = "general_assistance") -> Dict[str, Any]:
            """
            Process voice input with optional scene context.
            
            Args:
                voice_text: Recognized voice input text
                scene_context: Optional scene description for context
                template_type: Type of reasoning template to use
                
            Returns:
                Dictionary containing response and metadata
            """
            try:
                start_time = datetime.now()
                
                # Create interaction record
                interaction = {
                    'timestamp': start_time.isoformat(),
                    'input_type': 'voice',
                    'voice_text': voice_text,
                    'scene_context': scene_context,
                    'template_type': template_type,
                    'gemma_enabled': self.enable_gemma
                }
                
                # Generate response
                if self.enable_gemma and self.engine:
                    response = self._generate_gemma_response(
                        scene_context, voice_text, template_type
                    )
                    interaction['response_source'] = 'gemma3n'
                else:
                    response = self._generate_fallback_response(
                        scene_context, voice_text, template_type
                    )
                    interaction['response_source'] = 'fallback'
                
                # Finalize interaction record
                interaction['response'] = response
                interaction['processing_time'] = (datetime.now() - start_time).total_seconds()
                
                # Log interaction if enabled
                if self.log_interactions:
                    self._log_interaction(interaction)
                
                return {
                    'success': True,
                    'response': response,
                    'metadata': {
                        'source': interaction['response_source'],
                        'template_type': template_type,
                        'processing_time': interaction['processing_time']
                    }
                }
                
            except Exception as e:
                logger.error(f"Error in process_voice_input: {e}")
                return {
                    'success': False,
                    'response': self._get_error_response(str(e)),
                    'metadata': {
                        'source': 'error_handler',
                        'error': str(e)
                    }
                }
        
        def _generate_gemma_response(self,
                                   scene_description: str,
                                   user_question: str,
                                   template_type: str,
                                   additional_context: Optional[Dict[str, Any]] = None) -> str:
            """Generate response using Gemma3n."""
            try:
                # Build the prompt
                prompt = self.prompt_builder.build_prompt(
                    scene_description=scene_description,
                    user_question=user_question,
                    template_type=template_type,
                    additional_context=additional_context
                )
                
                # Log prompt if debugging enabled
                if self.log_interactions:
                    logger.debug(f"Generated prompt ({template_type}): {prompt[:200]}...")
                
                # Generate response
                response = self.engine.generate_response(prompt)
                
                # Validate response
                if not response or response.startswith("Error:") or response.startswith("Connection Error:"):
                    logger.warning(f"Gemma3n returned error response: {response[:100]}...")
                    return self._generate_fallback_response(scene_description, user_question, template_type, response)
                
                return response
                
            except Exception as e:
                logger.error(f"Error generating Gemma response: {e}")
                return self._generate_fallback_response(scene_description, user_question, template_type)
        
        def _generate_fallback_response(self,
                                      scene_description: str,
                                      user_question: str,
                                      template_type: str,
                                      error_response: str = None) -> str:
            """Generate fallback response when Gemma3n is unavailable."""
            logger.info("Using fallback response system")
            
            # Basic response construction based on input
            response_parts = []
            
            if scene_description.strip():
                if template_type == "vision_description":
                    response_parts.append(f"I can see: {scene_description}")
                    
                    # Add safety considerations
                    if any(word in scene_description.lower() for word in ['stairs', 'step', 'edge', 'drop']):
                        response_parts.append("Please be careful of elevation changes.")
                    
                    if any(word in scene_description.lower() for word in ['glass', 'fragile', 'sharp']):
                        response_parts.append("I notice some fragile items - please move carefully.")
                        
                else:
                    response_parts.append(f"Based on what I can observe: {scene_description}")
            
            if user_question.strip():
                question_lower = user_question.lower()
                
                if any(word in question_lower for word in ['help', 'what', 'how']):
                    if template_type == "navigation_help":
                        response_parts.append("For safety, I recommend moving slowly and feeling ahead with your hands or mobility aid.")
                    else:
                        response_parts.append("I'm here to help. Please let me know what specific assistance you need.")
                
                elif any(word in question_lower for word in ['safe', 'danger', 'careful']):
                    response_parts.append("Your safety is the priority. Take your time and move carefully.")
                
                elif any(word in question_lower for word in ['where', 'location', 'find']):
                    response_parts.append("Based on the current scene, I'll do my best to help you locate what you're looking for.")
            
            # Default response if nothing specific was detected
            if not response_parts:
                response_parts.append("I'm here to assist you. The AI reasoning system is currently using basic responses, but I can still help with your questions.")
            
            # Add specific note based on error type
            if error_response and "timed out" in error_response.lower():
                response_parts.append("(Note: AI is processing slowly - try again or use simpler requests)")
            elif error_response and "connection" in error_response.lower():
                response_parts.append("(Note: AI service temporarily unavailable - using basic responses)")
            else:
                response_parts.append("(Note: Using simplified response mode - advanced AI reasoning temporarily unavailable)")
            
            return " ".join(response_parts)
        
        def _get_error_response(self, error_msg: str) -> str:
            """Generate user-friendly error response."""
            return "I'm experiencing some technical difficulties right now. Please try again in a moment, or let me know if you need immediate assistance."
        
        def _log_interaction(self, interaction: Dict[str, Any]):
            """Log interaction for debugging purposes."""
            # Add to history
            self.interaction_history.append(interaction)
            
            # Trim history if too long
            if len(self.interaction_history) > self.max_history_length:
                self.interaction_history = self.interaction_history[-self.max_history_length:]
            
            # Log key details
            logger.info(f"Interaction logged - Type: {interaction['input_type']}, "
                       f"Source: {interaction.get('response_source', 'unknown')}, "
                       f"Time: {interaction.get('processing_time', 0):.2f}s")
        
        def get_interaction_history(self, limit: int = 10) -> List[Dict[str, Any]]:
            """Get recent interaction history."""
            return self.interaction_history[-limit:]
        
        def clear_interaction_history(self):
            """Clear interaction history."""
            self.interaction_history.clear()
            logger.info("Interaction history cleared")
        
        def get_system_status(self) -> Dict[str, Any]:
            """Get current system status."""
            status = {
                'gemma_enabled': self.enable_gemma,
                'model_name': self.model_name,
                'total_interactions': len(self.interaction_history),
                'logging_enabled': self.log_interactions
            }
            
            if self.enable_gemma and self.engine:
                # Get engine stats
                engine_stats = self.engine.get_stats()
                status['engine_stats'] = engine_stats
                
                # Check connection
                is_connected, connection_msg = self.engine.verify_connection()
                status['connection_status'] = {
                    'connected': is_connected,
                    'message': connection_msg
                }
            else:
                status['connection_status'] = {
                    'connected': False,
                    'message': 'Gemma3n disabled - using fallback responses'
                }
            
            return status
        
        def toggle_gemma(self, enable: bool) -> bool:
            """Enable or disable Gemma3n reasoning."""
            old_status = self.enable_gemma
            self.enable_gemma = enable
            
            if enable and not self.engine:
                self.engine = get_gemma_engine(self.model_name)
            
            logger.info(f"Gemma3n toggled from {old_status} to {enable}")
            return True
    
    # Global instance for easy access
    _reasoning_layer = None
    
    def get_reasoning_layer(enable_gemma: bool = True) -> GemmaReasoningLayer:
        """Get or create global reasoning layer instance."""
        global _reasoning_layer
        if _reasoning_layer is None:
            _reasoning_layer = GemmaReasoningLayer(enable_gemma=enable_gemma)
        return _reasoning_layer
    
    # Test the reasoning layer if run directly
    if __name__ == "__main__":
        # Test the reasoning layer
        reasoning = GemmaReasoningLayer(enable_gemma=True)
        
        # Test vision input
        vision_result = reasoning.process_vision_input(
            vision_description="A table with a bottle and a glass. Chair on the right.",
            user_question="What should I do?",
            template_type="vision_description"
        )
        
        print("Vision Processing Result:")
        print(f"Success: {vision_result['success']}")
        print(f"Response: {vision_result['response']}")
        print(f"Metadata: {vision_result['metadata']}")
        
        # Test voice input
        voice_result = reasoning.process_voice_input(
            voice_text="Tell me what's around me",
            scene_context="Kitchen with various appliances and utensils",
            template_type="general_assistance"
        )
        
        print("\\nVoice Processing Result:")
        print(f"Success: {voice_result['success']}")
        print(f"Response: {voice_result['response']}")
        print(f"Metadata: {voice_result['metadata']}")
        
        # Show system status
        status = reasoning.get_system_status()
        print(f"\\nSystem Status: {status}")
----- [END OF reasoning_layer.py] -----

üîπ 19. user_profile_manager.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\user_profile_manager.py
----- [START OF user_profile_manager.py] -----
    """
    User Profile Manager for AI Assistant
    Manages user preferences, roles, and personalized system prompts
    """
    
    import json
    import os
    from datetime import datetime
    from typing import Dict, Optional, Any
    
    class UserProfileManager:
        """Manages user profiles with persistent storage and role-based personalization."""
        
        def __init__(self, profiles_file: str = "user_profiles.json"):
            self.profiles_file = profiles_file
            self.profiles = self._load_profiles()
            
            # Role-based system prompt templates
            self.role_prompts = {
                "Best Friend": {
                    "prompt": "You are speaking with {name}. They see you as their best friend. Be casual, supportive, and emotionally available. Use friendly language, emojis, and show genuine interest in their life. Be encouraging and maintain a warm, personal tone.",
                    "emoji": "üòä",
                    "tone": "casual",
                    "traits": ["supportive", "emotionally available", "encouraging", "warm"]
                },
                "Motivator": {
                    "prompt": "You are speaking with {name}. They see you as their motivator. Be energetic, uplifting, and goal-focused. Help them push through challenges, celebrate their wins, and keep them accountable. Use inspiring language and focus on achievements.",
                    "emoji": "üí™",
                    "tone": "energetic",
                    "traits": ["uplifting", "goal-focused", "inspiring", "accountable"]
                },
                "Female Friend": {
                    "prompt": "You are speaking with {name}. They see you as their female friend. Be caring, warm, understanding, and empathetic like a close girlfriend. Listen actively, provide emotional support, and engage in meaningful conversations about life, relationships, and feelings.",
                    "emoji": "üíï",
                    "tone": "caring",
                    "traits": ["empathetic", "understanding", "supportive", "emotionally intelligent"]
                },
                "Friend": {
                    "prompt": "You are speaking with {name}. They see you as their friend. Be helpful, kind, and approachable. Maintain a friendly but balanced tone, offer assistance when needed, and engage in pleasant conversations while being respectful and reliable.",
                    "emoji": "üôÇ",
                    "tone": "friendly",
                    "traits": ["helpful", "kind", "approachable", "reliable"]
                },
                "Guide": {
                    "prompt": "You are speaking with {name}. They see you as their guide. Be knowledgeable, patient, and instructional. Focus on helping them learn and grow, provide detailed explanations, and guide them through complex topics with wisdom and clarity.",
                    "emoji": "üß†",
                    "tone": "instructional",
                    "traits": ["knowledgeable", "patient", "wise", "educational"]
                }
            }
        
        def _load_profiles(self) -> Dict[str, Any]:
            """Load user profiles from JSON file."""
            try:
                if os.path.exists(self.profiles_file):
                    with open(self.profiles_file, 'r', encoding='utf-8') as f:
                        return json.load(f)
                return {}
            except (FileNotFoundError, json.JSONDecodeError):
                return {}
        
        def _save_profiles(self):
            """Save user profiles to JSON file."""
            try:
                with open(self.profiles_file, 'w', encoding='utf-8') as f:
                    json.dump(self.profiles, f, indent=2, ensure_ascii=False)
            except Exception as e:
                print(f"Error saving profiles: {e}")
        
        def create_user(self, name: str, role: str, user_id: Optional[str] = None) -> Dict[str, Any]:
            """Create a new user profile."""
            if not name or not role:
                raise ValueError("Name and role are required")
            
            if role not in self.role_prompts:
                raise ValueError(f"Invalid role. Must be one of: {list(self.role_prompts.keys())}")
            
            key = user_id if user_id else name.lower()
            now = datetime.now().isoformat()
            
            profile = {
                'name': name,
                'role': role,
                'created_at': now,
                'updated_at': now,
                'interaction_count': 1,
                'preferences': {
                    'use_emojis': True,
                    'voice_enabled': True,
                    'language': 'en'
                },
                'stats': {
                    'total_messages': 0,
                    'favorite_topics': [],
                    'last_active': now
                }
            }
            
            self.profiles[key] = profile
            self._save_profiles()
            
            return {
                'success': True,
                'user_id': key,
                'profile': profile
            }
        
        def get_user(self, identifier: str) -> Optional[Dict[str, Any]]:
            """Get user profile by name or ID."""
            key = identifier.lower()
            return self.profiles.get(key)
        
        def update_user_role(self, identifier: str, new_role: str) -> Dict[str, Any]:
            """Update user's role preference."""
            if new_role not in self.role_prompts:
                raise ValueError(f"Invalid role. Must be one of: {list(self.role_prompts.keys())}")
            
            key = identifier.lower()
            if key not in self.profiles:
                raise ValueError("User not found")
            
            old_role = self.profiles[key]['role']
            self.profiles[key]['role'] = new_role
            self.profiles[key]['updated_at'] = datetime.now().isoformat()
            self.profiles[key]['interaction_count'] += 1
            
            self._save_profiles()
            
            return {
                'success': True,
                'old_role': old_role,
                'new_role': new_role,
                'message': f'Role updated from "{old_role}" to "{new_role}"'
            }
        
        def get_system_prompt(self, identifier: str) -> str:
            """Generate personalized system prompt based on user's role preference."""
            user = self.get_user(identifier)
            if not user:
                return "You are a helpful AI assistant."
            
            role = user['role']
            name = user['name']
            
            if role in self.role_prompts:
                return self.role_prompts[role]['prompt'].format(name=name)
            else:
                return f"You are speaking with {name}. Be helpful and friendly."
        
        def get_role_info(self, role: str) -> Optional[Dict[str, Any]]:
            """Get information about a specific role."""
            return self.role_prompts.get(role)
        
        def increment_interaction(self, identifier: str):
            """Increment user's interaction count."""
            key = identifier.lower()
            if key in self.profiles:
                self.profiles[key]['interaction_count'] += 1
                self.profiles[key]['stats']['last_active'] = datetime.now().isoformat()
                self._save_profiles()
        
        def get_user_stats(self, identifier: str) -> Optional[Dict[str, Any]]:
            """Get user statistics and profile info."""
            user = self.get_user(identifier)
            if not user:
                return None
            
            return {
                'name': user['name'],
                'role': user['role'],
                'interaction_count': user.get('interaction_count', 0),
                'member_since': user.get('created_at'),
                'last_interaction': user.get('updated_at'),
                'role_emoji': self.role_prompts.get(user['role'], {}).get('emoji', 'ü§ñ'),
                'role_traits': self.role_prompts.get(user['role'], {}).get('traits', [])
            }
        
        def list_all_roles(self) -> Dict[str, Dict[str, Any]]:
            """Get all available roles and their information."""
            return self.role_prompts
        
        def export_user_data(self, identifier: str) -> Optional[Dict[str, Any]]:
            """Export all user data for backup/transfer purposes."""
            user = self.get_user(identifier)
            if not user:
                return None
            
            return {
                'profile': user,
                'export_date': datetime.now().isoformat(),
                'role_info': self.role_prompts.get(user['role'])
            }
        
        def detect_role_change_intent(self, message: str) -> Optional[str]:
            """Detect if user wants to change their role based on message content."""
            message_lower = message.lower().strip()
            
            # Common phrases that indicate role change intent
            change_phrases = [
                'change how i see you',
                'update my role',
                'change your role',
                'i want to see you as',
                'update how i see you',
                'change our relationship',
                'update our relationship',
                'be my',
                'act like my',
                'i want you to be'
            ]
            
            for phrase in change_phrases:
                if phrase in message_lower:
                    # Try to extract specific role from message
                    for role in self.role_prompts.keys():
                        if role.lower() in message_lower:
                            return role
                    return "role_change_detected"  # Generic role change intent
            
            return None
    
    # Create global instance
    user_manager = UserProfileManager()
    
    # Convenience functions for Flask integration
    def create_user_profile(name: str, role: str, user_id: Optional[str] = None):
        """Create user profile using global manager."""
        return user_manager.create_user(name, role, user_id)
    
    def get_user_profile(identifier: str):
        """Get user profile using global manager."""
        return user_manager.get_user(identifier)
    
    def update_user_role(identifier: str, new_role: str):
        """Update user role using global manager."""
        return user_manager.update_user_role(identifier, new_role)
    
    def get_personalized_prompt(identifier: str):
        """Get personalized system prompt using global manager."""
        return user_manager.get_system_prompt(identifier)
    
    def get_user_statistics(identifier: str):
        """Get user stats using global manager."""
        return user_manager.get_user_stats(identifier)
----- [END OF user_profile_manager.py] -----

üîπ 20. user_profiles.json
üìç Location: C:\Users\bindu\Desktop\Competation\google\user_profiles.json
----- [START OF user_profiles.json] -----
    {
      "utkarsh": {
        "name": "utkarsh",
        "role": "Female Friend",
        "created_at": "2025-07-20T18:57:09.160195",
        "updated_at": "2025-07-20T18:57:09.160195",
        "interaction_count": 1
      }
    }
----- [END OF user_profiles.json] -----

üîπ 21. vedix_core.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\vedix_core.py
----- [START OF vedix_core.py] -----
    """
    VediX - Friendly Offline AI Assistant Core
    ==========================================
    This module contains the core logic for VediX, a voice-activated AI assistant
    that works completely offline using Vosk for speech recognition.
    """
    
    import json
    import vosk
    import wave
    import pyaudio
    import threading
    import time
    import random
    from datetime import datetime
    import os
    import glob
    
    class VediXCore:
        def __init__(self, model_path="model/vosk-model-small-en-us-0.15"):
            """Initialize VediX with Vosk model"""
            self.model_path = model_path
            self.model = None
            self.rec = None
            self.is_listening = False
            self.greeting_done = False
            
            # VediX personality responses with Markdown formatting
            self.greetings = [
                "**Hello, Utkarsh!** How can I *help* you today?",
                "**Hi there, Utkarsh!** What can I do for you?",
                "**Hey Utkarsh!** I'm here and ***ready to help***!",
                "***Good to see you again, Utkarsh!*** What's on your *mind*?"
            ]
            
            self.fallback_responses = [
                "I *didn't get that*, could you **try again**?",
                "*Sorry*, I didn't understand. Can you **repeat that**?",
                "I'm *not sure* what you meant. Could you say that **again**?",
                "*Hmm*, I didn't catch that. ***One more time*** please?"
            ]
            
            self.jokes = [
                "Why don't scientists trust atoms? Because they ***make up everything***!",
                "I told my computer a joke about **UDP**... but it *didn't get it*.",
                "Why did the robot go to therapy? It had too many ***bugs***!",
                "What do you call a computer superhero? A ***screen saver***!"
            ]
            
            self.time_responses = [
                "The **current time** is",
                "*Right now* it's",
                "It's ***currently***",
                "The **time** is"
            ]
            
            # Initialize Vosk model
            self.initialize_model()
        
        def initialize_model(self):
            """Initialize the Vosk model for offline speech recognition"""
            try:
                if not os.path.exists(self.model_path):
                    print(f"‚ùå Vosk model not found at {self.model_path}")
                    return False
                
                vosk.SetLogLevel(-1)  # Reduce Vosk logging
                self.model = vosk.Model(self.model_path)
                print("‚úÖ VediX: Vosk model loaded successfully!")
                return True
            except Exception as e:
                print(f"‚ùå VediX: Error loading Vosk model: {e}")
                return False
        
        def get_greeting(self):
            """Get a random greeting message"""
            if not self.greeting_done:
                self.greeting_done = True
                return random.choice(self.greetings)
            return random.choice(["What ***else*** can I help you with?", "*Anything else*, Utkarsh?"])
        
        def process_voice_command(self, text):
            """Process voice command and return appropriate response"""
            if not text or text.strip() == "":
                return random.choice(self.fallback_responses)
            
            text_lower = text.lower().strip()
            
            # First handle some quick local responses for speed with Markdown formatting
            if any(word in text_lower for word in ["hello", "hi", "hey"]):
                return "**Hello there!** *Great* to hear from you! How can I ***assist*** you today?"
            
            if any(word in text_lower for word in ["time", "what time", "current time"]):
                current_time = datetime.now().strftime("%I:%M %p")
                return f"The **current time** is ***{current_time}***"
            
            # For everything else, use Gemma
            try:
                from gemma import chat  # Import Gemma's chat function
                print(f"VediX: Sending to Gemma: '{text}'")
                
                # Use proper parameter names as defined in gemma.py
                response = chat(
                    prompt=text, 
                    context="", 
                    system_message="You are a helpful AI assistant. Provide clear, concise responses."
                )
                
                print(f"VediX: Gemma response: '{response}'")
                return response if response else random.choice(self.fallback_responses)
                
            except ImportError as e:
                print(f"VediX: Import error: {e}")
                return random.choice(self.fallback_responses)
            except Exception as e:
                print(f"VediX: Error calling Gemma: {e}")
                return random.choice(self.fallback_responses)
        
        def recognize_from_audio_data(self, audio_data):
            """Recognize speech from audio data using Vosk"""
            if not self.model:
                return ""
            
            try:
                rec = vosk.KaldiRecognizer(self.model, 16000)
                if rec.AcceptWaveform(audio_data):
                    result = json.loads(rec.Result())
                    return result.get('text', '')
                else:
                    partial = json.loads(rec.PartialResult())
                    return partial.get('partial', '')
            except Exception as e:
                print(f"VediX Recognition Error: {e}")
                return ""
        
        def find_local_music(self, music_dir="music"):
            """Find local music files"""
            music_extensions = ['*.mp3', '*.wav', '*.m4a', '*.flac', '*.ogg']
            music_files = []
            
            if os.path.exists(music_dir):
                for extension in music_extensions:
                    music_files.extend(glob.glob(os.path.join(music_dir, extension)))
            
            return music_files
    
    # Global VediX instance
    vedix_instance = None
    
    def get_vedix():
        """Get or create VediX instance"""
        global vedix_instance
        if vedix_instance is None:
            vedix_instance = VediXCore()
        return vedix_instance
----- [END OF vedix_core.py] -----

üîπ 22. voice_backend.py
üìç Location: C:\Users\bindu\Desktop\Competation\google\voice_backend.py
----- [START OF voice_backend.py] -----
    from flask import Flask, request, jsonify
    from flask_cors import CORS
    import tempfile
    import os
    import wave
    import json
    from vosk import Model, KaldiRecognizer
    import subprocess
    from vedix_core import get_vedix
    
    app = Flask(__name__)
    CORS(app)
    
    # Initialize VediX
    vedix = get_vedix()
    
    # Load Vosk Model for English (adjust path if necessary)
    vosk_model_path = "model/vosk-model-small-en-us-0.15"
    model = Model(vosk_model_path)
    recognizer = KaldiRecognizer(model, 16000)
    
    @app.route('/api/voice-interact', methods=['POST'])
    def voice_interact():
        """Process voice input with VediX offline AI assistant"""
        if 'audio' not in request.files:
            return jsonify(success=False, error="No audio file provided"), 400
        
        audio_file = request.files['audio']
        
        # Save uploaded audio to a temp file
        fd, temp_audio_path = tempfile.mkstemp(suffix='.webm')
        try:
            audio_file.save(temp_audio_path)
            
            # Convert webm to wav
            temp_wav_path = tempfile.mktemp(suffix='.wav')
            command = ['ffmpeg', '-y', '-i', temp_audio_path, '-ar', '16000', '-ac', '1', '-f', 'wav', temp_wav_path]
            subprocess.run(command, check=True)
            
            # Recognize using Vosk
            with wave.open(temp_wav_path, "rb") as wf:
                wf_content = wf.readframes(wf.getnframes())
                if recognizer.AcceptWaveform(wf_content):
                    result = recognizer.Result()
                    recognized_data = json.loads(result)
                    recognized_text = recognized_data.get('text', '')
                else:
                    return jsonify(success=False, error="No recognizable speech"), 400
            
            # Process with VediX
            vedix_response = vedix.process_voice_command(recognized_text)
            return jsonify(
                success=True, 
                reply=vedix_response,
                recognized_text=recognized_text,
                vedix_active=True
            )
    
        except subprocess.CalledProcessError as e:
            return jsonify(success=False, error="Error converting audio"), 500
    
        except Exception as e:
            return jsonify(success=False, error=str(e)), 500
        
        finally:
            os.close(fd)
            os.remove(temp_audio_path)
            if os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)
    
    @app.route('/api/status')
    def status():
        return jsonify(connected=True, message="Voice backend running!")
    
    if __name__ == '__main__':
        app.run(port=5000, debug=True)
    
----- [END OF voice_backend.py] -----

=== ‚úÖ End of Export ===
